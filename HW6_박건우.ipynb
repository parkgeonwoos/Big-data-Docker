{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CU7_YA2w22HL"
   },
   "source": [
    "# Using SQL queries on DataFrames\n",
    "\n",
    "- - -\n",
    "\n",
    "#### Declarative Manipulation (SQL)\n",
    "* Advantage: You need to describe only **what** is the result you want.\n",
    "* Disadvantage: SQL does not have primitives for common analysis operations such as **covariance**\n",
    "\n",
    "\n",
    "- - -\n",
    "\n",
    "* Spark supports a subset of the Hive SQL query language.\n",
    "* For example, You can use Hive select syntax to select a subset of the rows in a dataframe.\n",
    "* To use sql on a dataframe you need to ``first register it as a TempTable``\n",
    "* For variety, we are using here a small dataframe loaded from a JSON file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vCyt-7L22HQ"
   },
   "source": [
    "### pyspark import & SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nxi7wn022HS",
    "outputId": "c7ca9747-e890-4fe1-cd43-ea6994ffc9c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f8dddd7bb38>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "sc = SparkContext(master=\"local[*]\")\n",
    "print(sc)\n",
    "\n",
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mQflZ9j022HZ"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# people.json(예제파일 다운로드)\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1TZyM7Gfc6XWLot-L36TDV-JwySgHxGv4\", \"people.json\")\n",
    "data_file = \"./people.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGCXfhw122Hd",
    "outputId": "7b6903b8-646b-482f-b0dd-ff17f6365fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(data_file)\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method~\n",
    "people.show()\n",
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f14VbfKm22Hg"
   },
   "source": [
    "## ★Register the DataFrame as a table★\n",
    "\n",
    "* **Registers a DataFrame as a Temporary Table in the SQLContext**\n",
    "* Usage : ``registerTempTable(tableName)``\n",
    "* Arguments :\n",
    "```\n",
    "talbeName : A character vector containing the name of the table\n",
    "```\n",
    "* Example : [DataFrame_Spark].registerTempTable(\"hi_hi_hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Crg9dN_22Hh",
    "outputId": "7f480331-ddae-4f1f-b878-4031f84bcdcd",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show를 사용하면....\n",
      "+------+\n",
      "|  name|\n",
      "+------+\n",
      "|Justin|\n",
      "+------+\n",
      "\n",
      "\n",
      "collect를 사용하면....\n",
      "[Row(name='Justin')]\n"
     ]
    }
   ],
   "source": [
    "## Register this DataFrame as a table\n",
    "people.registerTempTable(\"people\")\n",
    "\n",
    "## SQL statements can be run by using the sql methods provided by sqlContext\n",
    "teenagers = sqlContext.sql(\"SELECT name FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# show를 사용했을 때, DataFrame return\n",
    "print(\"show를 사용하면....\")\n",
    "teenagers.show()\n",
    "\n",
    "# collect를 사용했을 때, Row형태의 RDD return\n",
    "print(\"\\ncollect를 사용하면....\")\n",
    "print(teenagers.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O_AMJyod22Hj"
   },
   "source": [
    "### Excercise 1 - lec6 복습!!(20 point)\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "* **출력**과 같은 값이 나오도록 code를 작성하세요 (개당 4 point)\n",
    "- - -\n",
    "**출력**\n",
    "\n",
    "```\n",
    "1 ====>  [Row(name='Justin')]\n",
    "2 ====>  Justin\n",
    "3 ====>  Row(name='Justin')\n",
    "4 ====> \n",
    "+---+----+\n",
    "|age|name|\n",
    "+---+----+\n",
    "| 30|Andy|\n",
    "+---+----+\n",
    "\n",
    "5 ====> \n",
    "+---+------+\n",
    "|age|  name|\n",
    "+---+------+\n",
    "| 30|  Andy|\n",
    "| 19|Justin|\n",
    "+---+------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-1Kycw722Hk",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ====>  [Row(name='Justin')]\n",
      "2 ====>  Justin\n",
      "3 ====>  Row(name='Justin')\n",
      "4 ====> \n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "5 ====> \n",
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 30|  Andy|\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "print('1 ====> ', teenagers.collect())#첫 열을 출력\n",
    "print('2 ====> ',teenagers.collect()[0]['name'])#첫 열의 name부분을 출력\n",
    "print('3 ====> ', teenagers.collect()[0])#첫 열의 원소 0번쨰를 출력\n",
    "print(\"4 ====> \") #filter함수를 통해 age라는 행에서 30인 사람만 출력\n",
    "people.filter(col(\"age\") == 30).show()\n",
    "print(\"5 ====> \") #filter함수를 통해 age라는 행에서 1이상인 사람만 출력\n",
    "people.filter(col(\"age\") > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bpbs68YX22Hn"
   },
   "source": [
    "## SQL Basic\n",
    "\n",
    "---\n",
    "\n",
    "SQL에 대해 기본적인 사용법을 학습합니다.\n",
    "\n",
    "* SQL 구문\n",
    "```\n",
    "SELECT [열] \n",
    "FROM [테이블] \n",
    "WHERE [조건]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_lpV5JH22Ho",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in /opt/conda/lib/python3.7/site-packages (0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OMIMySvH22Hq",
    "outputId": "5c4176d3-1125-47db-d1aa-0f029d1d591b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1hAHV6vC6FvVgrYnoN-lR-IfH488-H121 into ./NY.tgz... Done.\n",
      "NY.parquet/\n",
      "NY.parquet/_SUCCESS\n",
      "NY.parquet/part-00022-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00000-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00021-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00001-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00023-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00002-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00024-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00003-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00025-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00004-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00027-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00005-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00006-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00007-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00008-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00009-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00010-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00011-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00012-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00013-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00014-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00015-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00016-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00017-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00018-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00019-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00020-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n",
      "NY.parquet/part-00026-89caf7c0-9733-40ec-a650-7f368529dd01-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "# NY weather download & register\n",
    "from os.path import exists\n",
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import tarfile\n",
    "\n",
    "if exists(\"./NY.tgz\"):\n",
    "    !rm -rf ./NY.tgz\n",
    "if exists(\"./NY.parquet\"):\n",
    "    !rm -rf ./NY.parquet\n",
    "    \n",
    "gdd.download_file_from_google_drive(file_id='1hAHV6vC6FvVgrYnoN-lR-IfH488-H121',\n",
    "                                   dest_path = './NY.tgz')\n",
    "!tar -xzvf NY.tgz\n",
    "\n",
    "# read & register\n",
    "df = sqlContext.read.load(\"NY.parquet\")\n",
    "df.registerTempTable(\"weather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B4E_u0B822Hu"
   },
   "source": [
    "### (1) DESC\n",
    "* 테이블 구조 확인\n",
    "* ``[DataFrame].printSchema``와 동일한 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w7TEcV1r22Hx",
    "outputId": "0ab4ffe6-1881-4bc6-eb5e-b9573cade15c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|    Station|   string|   null|\n",
      "|Measurement|   string|   null|\n",
      "|       Year|   bigint|   null|\n",
      "|     Values|   binary|   null|\n",
      "| dist_coast|   double|   null|\n",
      "|   latitude|   double|   null|\n",
      "|  longitude|   double|   null|\n",
      "|  elevation|   double|   null|\n",
      "|      state|   string|   null|\n",
      "|       name|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.printSchema()\n",
    "\n",
    "# SQL code\n",
    "query= \"\"\"DESC weather\"\"\"#큰 따옴표 무조건 3개씩 작성할 것\n",
    "sqlContext.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "512UtbTx22H1"
   },
   "source": [
    "### (2) FIRST와 LAST\n",
    "* ``FIRST`` : 첫 번째 ROW\n",
    "* ``LAST`` : 마지막 ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSfjbSQ322H2",
    "outputId": "c200c4cf-558b-4774-ba08-b27696839824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|first(Station, false)|last(Station, false)|\n",
      "+---------------------+--------------------+\n",
      "|          USW00094704|         USC00307664|\n",
      "+---------------------+--------------------+\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+---------------------+--------------------+\n",
      "|first(Station, false)|last(Station, false)|\n",
      "+---------------------+--------------------+\n",
      "|          USW00094704|         USC00307664|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "from pyspark.sql.functions import col, first, last\n",
    "df.select(first(col(\"Station\")), last(col(\"Station\"))).show()\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT FIRST(Station), LAST(Station) FROM weather\"\"\"\n",
    "sqlContext.sql(query).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-8j_RdI22H5"
   },
   "source": [
    "### (3) SELECT와 AS\n",
    "* ``SELECT`` : 특정 column 선택\n",
    "* ``AS``     : 특정 column의 이름 변경\n",
    "* 사용팁 : ``select``와 동시에 ``as``를 이용하여 column의 이름을 변경한다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7-SR426522H6",
    "outputId": "063f068c-1f6e-4d10-d489-95eadc4137cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|JINOO_Station|          name_JW|\n",
      "+-------------+-----------------+\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+-------------+-----------------+\n",
      "|JINOO_Station|          name_JW|\n",
      "+-------------+-----------------+\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "|  USW00094704|DANSVILLE MUNI AP|\n",
      "+-------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.select(col(\"Station\").alias(\"JINOO_Station\"), col(\"name\").alias(\"name_JW\")).show(3)\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT Station AS JINOO_Station, name AS name_JW FROM weather\"\"\"\n",
    "sqlContext.sql(query).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S3qIEOQJ22H9"
   },
   "source": [
    "### (4) SELECT와 DISTINCT\n",
    "* ``SELECT`` : 특정 column 선택\n",
    "* ``DISTICNT``     : 결과값에 중복된 데이터가 있으면 중복되는 N개의 데이터는 1건으로 처리해서 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hX09OXGq22H-",
    "outputId": "f62141ab-a32a-4aa8-bdb5-7e4d7d9b2bac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------------+\n",
      "|JINOO_Station_DISTINCT|             name_JW|\n",
      "+----------------------+--------------------+\n",
      "|           USC00300505|       BEAVER MEADOW|\n",
      "|           USC00305426|         MOHONK LAKE|\n",
      "|           USC00305798|NEW YORK BENSONHURST|\n",
      "+----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+----------------------+--------------------+\n",
      "|JINOO_Station_DISTINCT|             name_JW|\n",
      "+----------------------+--------------------+\n",
      "|           USC00300505|       BEAVER MEADOW|\n",
      "|           USC00305426|         MOHONK LAKE|\n",
      "|           USC00305798|NEW YORK BENSONHURST|\n",
      "+----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.select(col(\"Station\").alias(\"JINOO_Station_DISTINCT\"), col(\"name\").alias(\"name_JW\")).distinct().show(3)\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT DISTINCT Station AS JINOO_Station_DISTINCT, name AS name_JW FROM weather\"\"\"\n",
    "sqlContext.sql(query).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FCzBjGFp22IA"
   },
   "source": [
    "### (5) SELECT와 COUNT, AVG, SUM, MAX, MIN\n",
    "* 이름 그대로의 역할!!\n",
    "* SELECT와 연계하여 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oW3AiVmh22IB",
    "outputId": "07534585-4515-4591-d236-17c822da6fb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(Station)|\n",
      "+--------------+\n",
      "|        168398|\n",
      "+--------------+\n",
      "\n",
      "+------------------+-------------------+\n",
      "|   avg(dist_coast)|    sum(dist_coast)|\n",
      "+------------------+-------------------+\n",
      "|245.78455113006692|4.138962684120101E7|\n",
      "+------------------+-------------------+\n",
      "\n",
      "+------------------+-----------------+\n",
      "|    min(elevation)|   max(elevation)|\n",
      "+------------------+-----------------+\n",
      "|-999.9000244140625|838.2000122070312|\n",
      "+------------------+-----------------+\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+--------------+\n",
      "|count(Station)|\n",
      "+--------------+\n",
      "|        168398|\n",
      "+--------------+\n",
      "\n",
      "+------------------+-------------------+\n",
      "|   avg(dist_coast)|    sum(dist_coast)|\n",
      "+------------------+-------------------+\n",
      "|245.78455113006692|4.138962684120101E7|\n",
      "+------------------+-------------------+\n",
      "\n",
      "+------------------+-----------------+\n",
      "|    min(elevation)|   max(elevation)|\n",
      "+------------------+-----------------+\n",
      "|-999.9000244140625|838.2000122070312|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "from pyspark.sql.functions import count, avg, sum, max, min\n",
    "df.select(count(col(\"Station\"))).show()\n",
    "df.select(avg(col(\"dist_coast\")), sum(col(\"dist_coast\"))).show()\n",
    "df.select(min(col(\"elevation\")), max(col(\"elevation\"))).show()\n",
    "\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query1 = \"\"\"SELECT COUNT(Station) FROM weather\"\"\"\n",
    "query2 = \"\"\"SELECT AVG(dist_coast), SUM(dist_coast) FROM weather\"\"\"\n",
    "query3 = \"\"\"SELECT MIN(elevation), MAX(elevation) FROM weather\"\"\"\n",
    "\n",
    "sqlContext.sql(query1).show(3)\n",
    "sqlContext.sql(query2).show(3)\n",
    "sqlContext.sql(query3).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8LbNVfC22ID"
   },
   "source": [
    "### (6) WHERE\n",
    "* **조건절** \n",
    "* pyspark의 ``filter``와 동일한 기능\n",
    "* example\n",
    "\n",
    "```\n",
    "# Example 1: weather 테이블에서 elevationrk가 800이상인 모든 column을 가져오되 중복을 제거해라\n",
    "SELECT DISTINCT * FROM weather WHERE elevation >= 800\n",
    "\n",
    "# Example 2: weather 테이블에서 elevation가 100 미만인 Station column을 가져와라\n",
    "SELECT Station FROM weather WHERE elevation < 100\n",
    "\n",
    "# Example 3: weather 테이블에서 elevation가 600이상이고 800 미만인 모든 column을 가져오되 중복을 제거해라!!\"\n",
    "SELECT DISTINCT * FROM weather WHERE elevation >= 600 AND elevation < 800\n",
    "\n",
    "```\n",
    "\n",
    "* example 외에도 사용법이 너무 많습니다. [눌러1](https://loveiskey.tistory.com/58), [눌러2](http://www.sqlprogram.com/Basics/sql-where.aspx), [눌러3](https://www.guru99.com/where-clause.html)을 참고해주세요.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeU0aFP922IE",
    "outputId": "5860b9b9-4d9c-4eea-a5f6-ee07c867b005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 : weather 테이블에서 elevationrk가 800이상인 모든 column을 가져오되 중복을 제거해라\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+-----------+\n",
      "|    Station|Measurement|Year|              Values|        dist_coast|          latitude|         longitude|        elevation|state|       name|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+-----------+\n",
      "|USC00307799|   TMAX_s20|2005|[02 C9 8F C9 0C C...| 128.7790069580078|42.016700744628906|-74.41670227050781|807.7000122070312|   NY|  SLIDE MTN|\n",
      "|USC00304520|       SNOW|1922|[00 00 00 00 00 0...|285.82501220703125|44.099998474121094|             -74.0|838.2000122070312|   NY|LAKE COLDEN|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Example 2: weather 테이블에서 elevation가 100 미만인 Station column을 가져와라\n",
      "+-----------+\n",
      "|    Station|\n",
      "+-----------+\n",
      "|USC00309055|\n",
      "|USC00309055|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Example 3: weather 테이블에서 elevation가 600이상이고 800 미만인 모든 column을 가져오되 중복을 제거해라!!\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+------------+\n",
      "|    Station|Measurement|Year|              Values|        dist_coast|          latitude|         longitude|        elevation|state|        name|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+------------+\n",
      "|USC00303124|   PRCP_s20|1993|[CE 4F C2 4F B0 4...|   349.06201171875| 42.70000076293945| -77.4000015258789|            602.0|   NY|GANNETT HILL|\n",
      "|USC00302366|   PRCP_s20|2001|[CA 50 B2 50 9B 5...|141.85299682617188|42.235599517822266|-74.14330291748047|606.9000244140625|   NY| EAST JEWETT|\n",
      "+-----------+-----------+----+--------------------+------------------+------------------+------------------+-----------------+-----+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL code\n",
    "print(\"Example 1 : weather 테이블에서 elevationrk가 800이상인 모든 column을 가져오되 중복을 제거해라\")\n",
    "query1 = \"\"\"SELECT DISTINCT * FROM weather WHERE elevation >= 800\"\"\"\n",
    "sqlContext.sql(query1).show(2)\n",
    "\n",
    "print(\"Example 2: weather 테이블에서 elevation가 100 미만인 Station column을 가져와라\")\n",
    "query2 = \"\"\"SELECT Station FROM weather WHERE elevation < 100\"\"\"\n",
    "sqlContext.sql(query2).show(2)\n",
    "\n",
    "print(\"Example 3: weather 테이블에서 elevation가 600이상이고 800 미만인 모든 column을 가져오되 중복을 제거해라!!\")\n",
    "query3 = \"\"\"SELECT DISTINCT * FROM weather WHERE elevation >= 600 AND elevation < 800\"\"\"\n",
    "sqlContext.sql(query3).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UAHQbOz22IG"
   },
   "source": [
    "### (7) GROUP BY와 HAVING\n",
    "* GROUP BY문은 동일한 값을 가진 데이터를 집계해서 조회하고자 할 때 사용하는 문장이다.\n",
    "* 작성 방법\n",
    "\n",
    "```\n",
    "SELECT  [구문]\n",
    "FROM    [구문]\n",
    "GROUP BY [구문]\n",
    "HAVING  [구문]\n",
    "\n",
    "- 집계할 컬럼을 GROUP BY절 뒤에 적어준다.\n",
    "- SELECT절에는 GROUP BY에 명시된 컬럼만 사용할 수 있다.\n",
    "- HAVING에는 GROUP BY에 조건을 추가한다\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHptWtTH22IH",
    "outputId": "b1d8cb7a-f934-43ac-9c8a-71d06e76398b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+\n",
      "|Measurement|count|MinYear|\n",
      "+-----------+-----+-------+\n",
      "|   TMIN_s20|13442|   1873|\n",
      "|       TMIN|13442|   1873|\n",
      "|   SNOW_s20|15629|   1884|\n",
      "|       TOBS|10956|   1876|\n",
      "|   SNWD_s20|14617|   1888|\n",
      "+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+-----------+-----+-------+\n",
      "|Measurement|count|MinYear|\n",
      "+-----------+-----+-------+\n",
      "|   TMIN_s20|13442|   1873|\n",
      "|       TMIN|13442|   1873|\n",
      "|   SNOW_s20|15629|   1884|\n",
      "|       TOBS|10956|   1876|\n",
      "|   SNWD_s20|14617|   1888|\n",
      "+-----------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pyspark code\n",
    "df.groupby(col(\"Measurement\")).agg({\"Measurement\" : \"count\", \"year\": \"min\"})\\\n",
    "  .withColumnRenamed(\"count(Measurement)\", \"count\")\\\n",
    "  .withColumnRenamed(\"min(year)\", \"MinYear\")\\\n",
    "  .show(5)\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT Measurement, COUNT(Measurement) AS count, MIN(year) AS MinYear\n",
    "           FROM weather\n",
    "           GROUP BY Measurement\"\"\"\n",
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ak9RJjO822IJ",
    "outputId": "172da101-effa-4930-fb05-0099fe4b7859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+\n",
      "|year|    avg(elevation)|   avg(dist_coast)|\n",
      "+----+------------------+------------------+\n",
      "|2013| 279.4017901454714|262.15733492754947|\n",
      "|2012|266.33072170511406| 257.8840703884677|\n",
      "|2011| 265.4277518365087|253.01556445820043|\n",
      "|2010|262.55130806776833|248.06859466340276|\n",
      "|2009| 261.5488291051326|246.52965264467915|\n",
      "+----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "★★★★★In SQL★★★★★★\n",
      "+----+------------------+------------------+\n",
      "|year|    avg(elevation)|   avg(dist_coast)|\n",
      "+----+------------------+------------------+\n",
      "|2013| 279.4017901454714|262.15733492754947|\n",
      "|2012|266.33072170511406| 257.8840703884677|\n",
      "|2011| 265.4277518365087|253.01556445820043|\n",
      "|2010|262.55130806776833|248.06859466340276|\n",
      "|2009| 261.5488291051326|246.52965264467915|\n",
      "+----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# year를 GROUP BY를 통해 그룹화 하고, year 별 dist_coast와 elevation의 평균을 구하고\n",
    "# year를 기준으로 오름차순 한다.\n",
    "from pyspark.sql.functions import desc\n",
    "df.groupby(col(\"year\")).agg({\"dist_coast\":\"avg\",\"elevation\":\"avg\"})\\\n",
    "  .sort(desc(\"year\")).show(5)\n",
    "\n",
    "\n",
    "# SQL code\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT year, AVG(elevation), AVG(dist_coast)\n",
    "           FROM weather\n",
    "           GROUP BY year\n",
    "           ORDER BY year DESC\n",
    "           \"\"\"\n",
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vIfDAGK_22IU",
    "outputId": "d745891f-8291-4808-eca7-f274608bc7d1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★★★★In SQL★★★★★★\n",
      "+----+------------------+------------------+\n",
      "|year|    avg(elevation)|   avg(dist_coast)|\n",
      "+----+------------------+------------------+\n",
      "|2013| 279.4017901454714|262.15733492754947|\n",
      "|2012|266.33072170511406| 257.8840703884677|\n",
      "|1915| 264.1221125600379|258.39760847437293|\n",
      "|1914|262.33999927043914| 259.3964216718078|\n",
      "|1913|271.95626452024027| 257.1068253879962|\n",
      "+----+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL code\n",
    "# HAVING 추가\n",
    "# year를 GROUP BY를 통해 그룹화 하고, year 별 dist_coast와 elevation의 평균을 구하고\n",
    "# dist_coast의 평균이 256 이상이고 elevation의 평균이 260 이상인 ROW를 구하되\n",
    "# year를 기준으로 오름차순 한다.\n",
    "print(\"★★★★★In SQL★★★★★★\")\n",
    "query = \"\"\"SELECT year, AVG(elevation), AVG(dist_coast)\n",
    "           FROM weather\n",
    "           GROUP BY year\n",
    "           HAVING AVG(dist_coast) >= 256 AND AVG(elevation) >= 260\n",
    "           ORDER BY year DESC\n",
    "           \"\"\"\n",
    "sqlContext.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwArms3l22IX"
   },
   "source": [
    "### ★★★★WHERE와 HAVING???★★★★\n",
    "\n",
    "query에 `WHERE`와 `HAVING`이 있다면, `WHERE`가 적용된 후*(`조건이 적용된 table`에)* `HAVING`이 적용된다.\n",
    "\n",
    "* **반드시 다음 주소에 접속하여 차이점을 확인할 것 (2와 3).**\n",
    "[http://wiki.gurubee.net/pages/viewpage.action?pageId=26743892](http://wiki.gurubee.net/pages/viewpage.action?pageId=26743892)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRUb74LW49Z9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SxRXO23_22IX"
   },
   "source": [
    "### Excercise 2 - GROUP BY와 HAVING 적용해보자! (30 point)\n",
    "\n",
    "- - -\n",
    "task 별로 ``GROUP BY``와 위에서 학습한 여러 ``SQL function``을 이용하여 query 를 작성하세요(task 별 10 point)\n",
    "\n",
    "---\n",
    "\n",
    "**task**\n",
    "\n",
    "* 1 : ``weather``에서 ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 가장 최근 ``year``의 이름은 ``maxYear``, 평균 ``dist_coast``와 ``elevation``의 이름은 ``avgDist_coast``, ``avgElevation``으로 변경하세요.(10 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 2 : ``weather``에서 ``year``가 ``2000 이상``인 결과에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "* 3 : ``weather``에서 ``year``가 ``2000 이상``인 결과에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구합니다. 그리고 ``name``별 평균 ``dist_coast``가 175.44400024414062이거나 평균 ``elevation``이 304.5인 결과를 출력하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)\n",
    "\n",
    "- - -\n",
    "**출력 예시**\n",
    "```\n",
    "# task1 output\n",
    "+-------------+-------+------------------+-----------------+\n",
    "|         name|maxYear|     avgDist_coast|     avgElevation|\n",
    "+-------------+-------+------------------+-----------------+\n",
    "|    ADAMS CTR|   1950|  376.802001953125|121.9000015258789|\n",
    "|      ADDISON|   2013| 296.1679992675781|            304.5|\n",
    "|ADDISON 1 NNE|   1988| 300.0060119628906|373.3999938964844|\n",
    "|       ALBANY|   1970|166.00100708007812|              0.0|\n",
    "|    ALBANY AP|   2013|175.44400024414062| 95.0999984741211|\n",
    "+-------------+-------+------------------+-----------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "# task2 output\n",
    "+-----------+-------+------------------+------------------+\n",
    "|       name|maxYear|     avgDist_coast|      avgElevation|\n",
    "+-----------+-------+------------------+------------------+\n",
    "|    ADDISON|   2013| 296.1679992675781|             304.5|\n",
    "|  ALBANY AP|   2013|175.44400024414062|  95.0999984741211|\n",
    "|ALBION 2 NE|   2012| 436.3030090332031|134.10000610351562|\n",
    "| ALCOVE DAM|   2013|152.88999938964844|             185.0|\n",
    "|     ALFRED|   2013| 330.6700134277344|             520.0|\n",
    "+-----------+-------+------------------+------------------+\n",
    "only showing top 5 rows\n",
    "\n",
    "# task3 output\n",
    "+---------+-------+------------------+----------------+\n",
    "|     name|maxYear|     avgDist_coast|    avgElevation|\n",
    "+---------+-------+------------------+----------------+\n",
    "|ALBANY AP|   2013|175.44400024414062|95.0999984741211|\n",
    "|  ADDISON|   2013| 296.1679992675781|           304.5|\n",
    "+---------+-------+------------------+----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I3hJeFsm22IY"
   },
   "source": [
    "**task**\n",
    "* 1 : ``weather``에서 ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 가장 최근 ``year``의 이름은 ``maxYear``, 평균 ``dist_coast``와 ``elevation``의 이름은 ``avgDist_coast``, ``avgElevation``으로 변경하세요.(10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yUg9sySf22Ia",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+------------------+-----------------+\n",
      "|         name|maxYear|     avgDist_coast|     avgElevation|\n",
      "+-------------+-------+------------------+-----------------+\n",
      "|    ADAMS CTR|   1950|  376.802001953125|121.9000015258789|\n",
      "|      ADDISON|   2013| 296.1679992675781|            304.5|\n",
      "|ADDISON 1 NNE|   1988| 300.0060119628906|373.3999938964844|\n",
      "|       ALBANY|   1970|166.00100708007812|              0.0|\n",
      "|    ALBANY AP|   2013|175.44400024414062| 95.0999984741211|\n",
      "+-------------+-------+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-1 답 작성\n",
    "task1_query = \"\"\"SELECT name, MAX(year) AS maxYear, AVG(dist_coast) AS avgDist_coast, \n",
    "                        AVG(elevation) AS avgElevation \n",
    "                 FROM weather\n",
    "                 GROUP BY name\n",
    "                 ORDER BY name\"\"\"\n",
    "#name과 year의 가장 최근을 뽑기 위해 max함수를 써주고 이름을 maxYear로 바꿈 또한 dist_coast행의 평균을 내고 avgDist_coast로 바꾸고 \n",
    "#그 자료들은 weather에서 가져오고 name별로 묶는다.\n",
    "\n",
    "# output\n",
    "sqlContext.sql(task1_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWkGp_lu22If"
   },
   "source": [
    "**task**\n",
    "* 2 : ``weather``에서 ``year``가 ``2000 이상``인 Row에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구하고 가장 최근 ``name``를 기준으로 정렬(default) 하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lS_-htFU22Ig",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------------------+------------------+\n",
      "|       name|maxYear|     avgDist_coast|      avgElevation|\n",
      "+-----------+-------+------------------+------------------+\n",
      "|    ADDISON|   2013| 296.1679992675781|             304.5|\n",
      "|  ALBANY AP|   2013|175.44400024414062|  95.0999984741211|\n",
      "|ALBION 2 NE|   2012| 436.3030090332031|134.10000610351562|\n",
      "| ALCOVE DAM|   2013|152.88999938964844|             185.0|\n",
      "|     ALFRED|   2013| 330.6700134277344|             520.0|\n",
      "+-----------+-------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-2 답 작성\n",
    "task2_query = \"\"\"SELECT name, MAX(year) AS maxYear, AVG(dist_coast) AS avgDist_coast, \n",
    "                        AVG(elevation) AS avgElevation \n",
    "                 FROM weather\n",
    "                 WHERE year >= 2000\n",
    "                 GROUP BY name\n",
    "                 ORDER BY name\"\"\"\n",
    "#name과 최근 해를 뽑기 위해 max함수를 써주고 maxYear로 이름을 바꿔주고 dist_coast의 평균을 구하고 avgDist_coast로 이름을 변경\n",
    "#elevation의 평균을 구해주고 avgElevation으로 변경 weahter로부터 데이터를 가져오고 year가 2000이상인 조건을 가지고 이름별로 묶는다\n",
    "# output\n",
    "sqlContext.sql(task2_query).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fhJwYAK22Ii"
   },
   "source": [
    "**task**\n",
    "* 3 : ``weather``에서 ``year``가 ``2000 이상``인 Row에 대해, ``name``별 가장 최근 ``year``와 ``dist_coast``, ``elevation``의 평균을 구합니다. 그리고 ``name``별 평균 ``dist_coast``가 175.44400024414062이거나 평균 ``elevation``이 304.5인 결과를 출력하는 query를 작성합니다. 단, 각 column명은 task1과 동일하게 적용합니다. (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-kpeR7rb22Ij",
    "outputId": "8ce84221-e0d1-4d54-a630-7413f855118a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------------+----------------+\n",
      "|     name|maxYear|     avgDist_coast|    avgElevation|\n",
      "+---------+-------+------------------+----------------+\n",
      "|ALBANY AP|   2013|175.44400024414062|95.0999984741211|\n",
      "|  ADDISON|   2013| 296.1679992675781|           304.5|\n",
      "+---------+-------+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2-3 답 작성\n",
    "task3_query = \"\"\"SELECT name, MAX(year) AS maxYear, AVG(dist_coast) AS avgDist_coast, \n",
    "                        AVG(elevation) AS avgElevation \n",
    "                 FROM weather\n",
    "                 WHERE year >= 2000\n",
    "                 GROUP BY name\n",
    "                 HAVING avgElevation == 304.5 OR avgDist_coast == 175.44400024414062\"\"\"\n",
    "#이름을 뽑고,최근 해를 가져오기 위해 max함수를 쓰고 maxYear로 이름을 바꾸고 dist_coast의 평균을 구하고 avgDist_coast로 이름을 바꾼다\n",
    "#elevation의 평균을 구하고 이름을 avgElevation으로 바꾸고 데이터는 weather에서 가져오고 2000년이상이 로우라는 조건을 걸고 name별로 이름을 건다\n",
    "#또한 조건으로 avgElevationdl 304.5dlrjsk avgDist_coast가 175.444...일경우의 조건을 건다.\n",
    "# output\n",
    "sqlContext.sql(task3_query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CO48H3xf22Il"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "me8hSgJM22Io"
   },
   "source": [
    "## Exercise 3 \n",
    "\n",
    "### DataFrame과 SQL을 사용하여 HW4 Exercise 3 다시 풀기! (25 point)\n",
    "\n",
    "- - -\n",
    " \n",
    "다음 데이터에 대하여 다음 과제를 수행하세요.\n",
    "\n",
    "- regular.csv : KBO에서 활약한 타자들의 역대 정규시즌 성적을 포함하여 몸무게, 키 ,생년월일 등의 기본정보\n",
    "\n",
    "**위의 데이터는 `,`로 구분되어 있습니다.**\n",
    "\n",
    " - **데이터의 자세한 설명은 다음의 링크를 참조해주세요.([여기를 눌러서 12. 데이터 설명 참고](https://dacon.io/cpt6/62885))**\n",
    " - 또한 regular.csv와 pre.csv를 직접 열어서 데이터가 어떻게 저장되어 있는지 확인해주세요.\n",
    "\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "- 1. sqlContext.read.csv를 이용하여 ``regular.csv``를 DataFrame으로 만든 후 . ``registerTempTable(\"task1_table\")``을 이용하여 table로 등록합니다. 그리고 SQL의 ``DESC``를 활용하여 구조를 확인합니다(5 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "- 2. task1에서 생성된 ``task1_table``에서, 타율(avg) column의 type을 String에서 Double로 변환 후 ``registerTempTable(\"task2_table\")``을 사용하여 새로운 table로 등록합니다(10 point)\n",
    "\n",
    "<br>\n",
    "\n",
    "- 3. task2에서 생성된 ``task2_table``에서 타율(avg)이 0.300을 초과하는 모든 선수에 대해(중복포함), **각 타자의 3할을 친 횟수가 8 이상인 타자의 이름(batter_name)과 3할을 친 횟수(avgCount)를 구합니다**. 이에 덧붙여서, 3할을 친 횟수를 기준으로 내림차순을 적용하는 query를 작성하고 출력합니다.(10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWMCAmMj22Ip"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from os.path import exists\n",
    "\n",
    "if exists(\"./regular.csv\"):\n",
    "    !rm -rf regular.csv\n",
    "\n",
    "f = urllib.request\\\n",
    ".urlretrieve (\"https://docs.google.com/uc?export=download&id=1t3icaDgI5KeNEwNmaWFOYGYQtdY8NOMm\",\n",
    "              \"regular.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k70cDUb022Is"
   },
   "source": [
    "### ★★★ table 관련해서 Error 발생시 ★★★\n",
    "\n",
    "아래의 코드를 이용해서, 등록 table을 지운 후 진행합시다.!\n",
    "\n",
    "```\n",
    "sqlContext.sql(\"SHOW tables\").show()\n",
    "sqlContext.dropTempTable(\"[지우고자 하는 table 이름]\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oo9pzNz922Is"
   },
   "source": [
    "**task**\n",
    "\n",
    "- 1. sqlContext.read.csv를 이용하여 ``regular.csv``를 DataFrame으로 만든 후 . ``registerTempTable(\"task1_table\")``을 이용하여 table로 등록합니다. 그리고 SQL의 ``DESC``를 활용하여 구조를 확인합니다(5 point)\n",
    "\n",
    "```\n",
    "# task1 output\n",
    "+-----------+---------+-------+\n",
    "|   col_name|data_type|comment|\n",
    "+-----------+---------+-------+\n",
    "|  batter_id|   string|   null|\n",
    "|batter_name|   string|   null|\n",
    "|       year|   string|   null|\n",
    "|       team|   string|   null|\n",
    "|        avg|   string|   null|\n",
    "|          G|   string|   null|\n",
    "|         AB|   string|   null|\n",
    "|          R|   string|   null|\n",
    "|          H|   string|   null|\n",
    "|         2B|   string|   null|\n",
    "|         3B|   string|   null|\n",
    "|         HR|   string|   null|\n",
    "|         TB|   string|   null|\n",
    "|        RBI|   string|   null|\n",
    "|         SB|   string|   null|\n",
    "|         CS|   string|   null|\n",
    "|         BB|   string|   null|\n",
    "|        HBP|   string|   null|\n",
    "|         SO|   string|   null|\n",
    "|        GDP|   string|   null|\n",
    "+-----------+---------+-------+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dan8iCO_22It"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+-------+\n",
      "|   col_name|data_type|comment|\n",
      "+-----------+---------+-------+\n",
      "|  batter_id|   string|   null|\n",
      "|batter_name|   string|   null|\n",
      "|       year|   string|   null|\n",
      "|       team|   string|   null|\n",
      "|        avg|   string|   null|\n",
      "|          G|   string|   null|\n",
      "|         AB|   string|   null|\n",
      "|          R|   string|   null|\n",
      "|          H|   string|   null|\n",
      "|         2B|   string|   null|\n",
      "|         3B|   string|   null|\n",
      "|         HR|   string|   null|\n",
      "|         TB|   string|   null|\n",
      "|        RBI|   string|   null|\n",
      "|         SB|   string|   null|\n",
      "|         CS|   string|   null|\n",
      "|         BB|   string|   null|\n",
      "|        HBP|   string|   null|\n",
      "|         SO|   string|   null|\n",
      "|        GDP|   string|   null|\n",
      "+-----------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.read.csv(\"regular.csv\", header = True).registerTempTable(\"task1_table\")\n",
    "#regular.csv를 읽어오고 읽은 자료를 task1_table이라는 이름으로 테이블을 만듦\n",
    "#output\n",
    "\n",
    "\n",
    "sqlContext.sql(\"DESC task1_table\").show()#정렬하여 테이블을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpTpjmRR22I0"
   },
   "source": [
    "**task**\n",
    "* 2. task1에서 생성된 ``task1_table``에서, 타율(avg) column의 type을 String에서 Double로 변환 후 ``registerTempTable(\"task2_table\")``을 사용하여 새로운 table로 등록합니다(10 point)\n",
    "\n",
    "```\n",
    "# SQL에서 column type 변경하기\n",
    "SELECT *, cast([column 이름] as [바꿀 type]) AS [새로운 column 이름] FROM [table]\n",
    "# 여기서 기존의 column은 제거되지 않고 type이 변경된 column이 추가됩니다.\n",
    "\n",
    "# task2 output\n",
    "Row(col_name='avg_double', data_type='double', comment=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7rEAyhJP22I2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(col_name='avg_double', data_type='double', comment=None)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3-2 답 작성\n",
    "a=\"\"\"SELECT *,cast(avg as Double) AS avg_double FROM task1_table\"\"\"#a변수에 모든 행을 뽑고 avg를 double형태로 바꿔주고 이름을 avg_double로 바꾼다 table은 task1_table에서 가져온다\n",
    "sqlContext.sql(a).registerTempTable(\"task2_table\")\n",
    "#a를 통해 새로운 테이블 task2_table을 만든다\n",
    "# output\n",
    "sqlContext.sql(\"DESC task2_table\").collect()[-1]#정렬하고 마지막 것만 뽑아낸다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrkMDFWz22I4"
   },
   "source": [
    "**task 3**\n",
    "- 3. task2에서 생성된 ``task2_table``에서 타율(avg)이 0.300을 초과하는 모든 선수에 대해(중복포함), **각 타자의 3할을 친 횟수가 8 이상인 타자의 이름(batter_name)과 3할을 친 횟수(avgCount)를 구합니다**. 이에 덧붙여서, 3할을 친 횟수를 기준으로 내림차순을 적용하는 query를 작성하고 출력합니다.(10 point)\n",
    "\n",
    "```\n",
    "# task3 output\n",
    "+-----------+--------+\n",
    "|batter_name|avgCount|\n",
    "+-----------+--------+\n",
    "|     김태균|      13|\n",
    "|     김주찬|      11|\n",
    "|     이진영|      11|\n",
    "|     이택근|      10|\n",
    "|     손아섭|      10|\n",
    "|     정근우|       9|\n",
    "|     장성호|       9|\n",
    "|     박용택|       9|\n",
    "|     김동주|       9|\n",
    "|     정성훈|       8|\n",
    "|     이대호|       8|\n",
    "|     최형우|       8|\n",
    "|     박한이|       8|\n",
    "|     김현수|       8|\n",
    "+-----------+--------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gVOlLXL22I6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|batter_name|avgCount|\n",
      "+-----------+--------+\n",
      "|     김태균|      13|\n",
      "|     이진영|      11|\n",
      "|     김주찬|      11|\n",
      "|     이택근|      10|\n",
      "|     손아섭|      10|\n",
      "|     김동주|       9|\n",
      "|     장성호|       9|\n",
      "|     정근우|       9|\n",
      "|     박용택|       9|\n",
      "|     정성훈|       8|\n",
      "|     이대호|       8|\n",
      "|     최형우|       8|\n",
      "|     김현수|       8|\n",
      "|     박한이|       8|\n",
      "+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3-3 답 작성\n",
    "query3 =  \"\"\"SELECT batter_name, COUNT(avg_double) AS avgCount\n",
    "FROM task2_table\n",
    "WHERE avg_double > 0.300\n",
    "GROUP BY batter_name\n",
    "HAVING avgCount >= 8\n",
    "ORDER BY avgCount DESC\"\"\"\n",
    "#batter_name과 avg_double를 카운트하고 avgCount로 이름을 바꾸고 task2_table에서 가져오며 avg_double>0.300이상이여야하고 batter_name으로 묶으며\n",
    "#avgCount>8이어야 하며 정렬을 한다.\n",
    "# output\n",
    "sqlContext.sql(query3).show()#sql조건 값을 적용하여 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BO2ZK5FG22I9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dmgy5aJm22JB"
   },
   "source": [
    "## Exercise 4 \n",
    "\n",
    "### 데이터를 찾고 DataFrame과 SQL을 적용해보자 (25 point)\n",
    "- - -\n",
    "\n",
    "지금까지 계속 똑같은 데이터(야구와 야구 그리고 baseball 등)를 사용하여 과제를 진행하였다. \n",
    "\n",
    "지겹지 않은가??? *(??? : 사실 내가 지겹다.)*\n",
    "\n",
    "<br>\n",
    "\n",
    "HW6 Exercise 4는 야구 데이터의 매너리즘에 빠진 당신을 위해 준비하였다. ㅋ\n",
    "\n",
    "- - -\n",
    "\n",
    "**task**\n",
    "* 1. 공공데이터포털([https://www.data.go.kr/](https://www.data.go.kr/)), 데이콘([https://dacon.io/](https://dacon.io/)), kaggle([https://www.kaggle.com/](https://www.kaggle.com/)]) 등 각종 데이터를 제공하는 사이트에 접속하여 데이터를 찾습니다. 데이터의 형식은 상관 X. ``하지만, 우리가 학습해본 csv, json, parquet가 편하지 않을까...`` **데이터를 찾은 후 `sqlContext의 read.[형식에 맞게]`를 적용하여 DataFrame으로 저장한다**. 그 후, ``registerTempTable``을 사용하여 ``table``로 등록후, SQL을 사용하여 데이터의 구조를 출력합니다. (10 point)\n",
    "\n",
    "    1. **[DataFrameReader 참고](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)**\n",
    "\n",
    "<br>\n",
    "\n",
    " \n",
    "* 2. task1에서 불러온 데이터에 대해 설명하시오. (5 point)\n",
    "\n",
    "    **<반드시 포함되어야할 내용>**\n",
    "    \n",
    "    1. 데이터의 출처, 데이터의 description, 데이터 활용 방안 등\n",
    "    \n",
    "<br>\n",
    "\n",
    "* 3. task1의 데이터를 활용하여 자유롭게 결과를 출력하고, 왜 그러한 결과를 출력했는지와 코드에 대한 설명을 추가하세요.(10 point)\n",
    "\n",
    "    **<반드시 포함되어야할 사항>**\n",
    "    \n",
    "    1. pyspark 함수가 아닌 **SQL 명령어 사용**. 또한, ``GROUP BY``, ``HAVING`` 반드시 포함되어야 함\n",
    "    2. 결과에 대한 설명(print로 작성하시오)\n",
    "    3. 주석 반드시 포함(코드에 대한 주석 #)\n",
    "    \n",
    "- - -\n",
    "**출력 예시**\n",
    "\n",
    "![png](http://www.artinsight.co.kr/data/news/1811/1988088618_t0JncQ9w_13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FhOSBQkZ22JC"
   },
   "source": [
    "**task**\n",
    "* 1. 공공데이터포털([https://www.data.go.kr/](https://www.data.go.kr/)), 데이콘([https://dacon.io/](https://dacon.io/)), kaggle([https://www.kaggle.com/](https://www.kaggle.com/)]) 등 각종 데이터를 제공하는 사이트에 접속하여 데이터를 찾습니다. 데이터의 형식은 상관 X. ``하지만, 우리가 학습해본 csv, json, parquet가 편하지 않을까...`` **데이터를 찾은 후 `sqlContext의 read.[형식에 맞게]`를 적용하여 DataFrame으로 저장한다**. 그 후, ``registerTempTable``을 사용하여 ``table``로 등록후, SQL을 사용하여 데이터의 구조를 출력합니다. (10 point)\n",
    "\n",
    "    1. **[DataFrameReader 참고](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader)**\n",
    "\n",
    "\n",
    "```\n",
    "sqlContext.read.csv(\"./전국금연구역표준데이터.csv\", header = True, encoding=\"EUC-kR\").registerTempTable(\"non_smoking\")\n",
    "\n",
    "★ task1 example output ★\n",
    "★ tempTable SHOW★\n",
    "+--------+-----------+-----------+\n",
    "|database|  tableName|isTemporary|\n",
    "+--------+-----------+-----------+\n",
    "|        |non_smoking|       true|\n",
    "|        |     people|       true|\n",
    "|        |task1_table|       true|\n",
    "|        |task2_table|       true|\n",
    "|        |    weather|       true|\n",
    "+--------+-----------+-----------+\n",
    "\n",
    "★ DESC★\n",
    "+----------------+---------+-------+\n",
    "|        col_name|data_type|comment|\n",
    "+----------------+---------+-------+\n",
    "|      금연구역명|   string|   null|\n",
    "|금연구역범위상세|   string|   null|\n",
    "|          시도명|   string|   null|\n",
    "|        시군구명|   string|   null|\n",
    "|    금연구역구분|   string|   null|\n",
    "+----------------+---------+-------+\n",
    "only showing top 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWIo8j1o22JD",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★ tempTable SHOW★\n",
      "+--------+-----------+-----------+\n",
      "|database|  tableName|isTemporary|\n",
      "+--------+-----------+-----------+\n",
      "|        |  acc_road1|       true|\n",
      "|        |    geonmoo|       true|\n",
      "|        |  one_table|       true|\n",
      "|        | one_table1|       true|\n",
      "|        |  one_talbe|       true|\n",
      "|        |task2_table|       true|\n",
      "+--------+-----------+-----------+\n",
      "\n",
      "★ DESC★\n",
      "+------------+---------+-------+\n",
      "|    col_name|data_type|comment|\n",
      "+------------+---------+-------+\n",
      "|        name|   string|   null|\n",
      "|        year|   string|   null|\n",
      "|    location|   string|   null|\n",
      "|locationcode|   string|   null|\n",
      "|locationname|   string|   null|\n",
      "+------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4-1 답안 작성\n",
    "# example code\n",
    "#sqlContext.read.csv(\"./전국금연구역표준데이터.csv\", header = True, encoding=\"EUC-kR\").registerTempTable(\"non_smoking\")\n",
    "sqlContext.read.csv(\"./accroad1.csv\", header = True, encoding=\"EUC-kR\").registerTempTable(\"acc_road1\")\n",
    "#내가 다운받은 accroad1의 csv를 읽고 acc_road1이라는 이름으로 테이블을 만듦\n",
    "\n",
    "\n",
    "\n",
    "# example output\n",
    "# table명은 자유롭게 \n",
    "print(\"★ tempTable SHOW★\")#tempTalbeshow를 출력\n",
    "sqlContext.sql(\"SHOW tables\").show()#SHOWTables을 통해 내가 가지고 있는 csv들을 출력\n",
    "print(\"★ DESC★\")#정렬\n",
    "sqlContext.sql(\"DESC acc_road1\").show(5)#내가 다운받은 acc_road1을 정렬하여 5개만 보여준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-VaeDKh22JG"
   },
   "source": [
    "**task**\n",
    "* 2. task1에서 불러온 데이터에 대해 설명하시오. (5 point)\n",
    "\n",
    "    **<반드시 포함되어야할 내용>**\n",
    "    \n",
    "    1. 데이터의 출처, 데이터의 description, 데이터 활용 방안 등"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-JpprF5S22JH",
    "outputId": "82311131-066a-44f1-fa80-03c03c34bc2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4-2 답안 작성] \n",
      "\n",
      "데이터의 출처 : https://www.data.go.kr/dataset/15003493/fileData.do\n",
      "데이터의 description : 도로교통공단에서 교통사고 다발지역에 따른 부상자수\n",
      "데이터 활용 방안 :이 데이터를 통해 사고다발 지역 구간에서 운전을 조심하도록 경각시킬 수 있음 \n",
      "기타 등등 : 그 외에도 위험한 지역을 정부가 파악하여 위험지역에 사고가 나지 않도록 인력배치 및 기구들을 설치할 수 있는 좋은 통계자료\n"
     ]
    }
   ],
   "source": [
    "# 4-2 답안 작성\n",
    "print(\"[4-2 답안 작성] \\n\")\n",
    "print(\"데이터의 출처 : https://www.data.go.kr/dataset/15003493/fileData.do\")\n",
    "print(\"데이터의 description : 도로교통공단에서 교통사고 다발지역에 따른 부상자수\")\n",
    "print(\"데이터 활용 방안 :이 데이터를 통해 사고다발 지역 구간에서 운전을 조심하도록 경각시킬 수 있음 \")\n",
    "print(\"기타 등등 : 그 외에도 위험한 지역을 정부가 파악하여 위험지역에 사고가 나지 않도록 인력배치 및 기구들을 설치할 수 있는 좋은 통계자료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iivwMSCO22JJ"
   },
   "source": [
    "**task**\n",
    "* 3. task1의 데이터를 활용하여 자유롭게 결과를 출력하고, 왜 그러한 결과를 출력했는지와 코드에 대한 설명을 추가하세요.(10 point)\n",
    "\n",
    "    **<반드시 포함되어야할 사항>**\n",
    "    \n",
    "    1. pyspark 함수가 아닌 **SQL 명령어 사용**. 또한, ``GROUP BY``, ``HAVING`` 반드시 포함되어야 함\n",
    "    2. 결과에 대한 설명(print로 작성하시오)\n",
    "    3. 주석 반드시 포함(코드에 대한 주석 #)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zgPr41mZ22JL"
   },
   "source": [
    "# 수고ㅋ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|   name|          avg_dead|\n",
      "+-------+------------------+\n",
      "|2019034|2.0714285714285716|\n",
      "|2016147| 5.668341708542713|\n",
      "|2014105| 4.021739130434782|\n",
      "|2014110|3.4430379746835444|\n",
      "|2016040| 2.046511627906977|\n",
      "+-------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "사고지역관리별 평균 사상자 수의 출력\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query1=\"\"\"SELECT name, avg(avg1) as avg_dead\n",
    "FROM acc_road1\n",
    "GROUP BY name \n",
    "HAVING avg(avg1) >= 1\n",
    "\"\"\" #acc_road1 테이블 사용 name,avg1의 평균을 avg_dead로 바꿔서 뽑아냄\n",
    "    #name별로 묶어주고 avg1의 평균이 1이상 큰 것을 출력\n",
    "sqlContext.sql(query1).show(5)#수가 많아지므로 5개만 출력해서 보여줌\n",
    "print(\"사고지역관리별 평균 사상자 수의 출력\")\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW6_upload_V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
