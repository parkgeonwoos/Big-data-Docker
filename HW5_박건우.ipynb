{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KiC6cm3phVRZ"
   },
   "source": [
    "# DataFrame\n",
    "\n",
    " - - -\n",
    "\n",
    "이번 notebook에서는 Spark RDD의 sub-type인 DataFrame에 대해 학습을 진행합니다.\n",
    "\n",
    "* Dataframes are a restricted sub-type of RDDs. \n",
    "* Restircing the type allows for more optimization.\n",
    "* Dataframes store two dimensional data, similar to the type of data stored in a spreadsheet(ex. MS Excel). \n",
    "   * Each column in a dataframe can have a different type.\n",
    "   * Each row contains a `record`.\n",
    "   \n",
    "**Spark의 DataFrame은 python pandas의 DataFrame, R의 DataFrame과 매우 유사합니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41Wx6xo9hVRc"
   },
   "source": [
    "### pyspark import & SparkContext 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rC2rKUjEhVRe",
    "outputId": "ec2a938b-f2b2-4217-c739-30877fcf6653"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<pyspark.sql.context.SQLContext object at 0x7f951c3889b0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType\n",
    "\n",
    "sc = SparkContext(master=\"local[*]\")\n",
    "print(sc)\n",
    "\n",
    "# Just like using Spark requires having a SparkContext, using SQL requires an SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "print(sqlContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5gGmaxO0hVRj"
   },
   "source": [
    "### Constructing a DataFrame from an RDD of Rows\n",
    "Each Row defines it's own  fields, the schema is *inferred*.\n",
    "\n",
    "여기서, ``schema is inferred``는 Spark가 스스로 각 column의 데이터의 유형을 파악하고, 그에 맞는 형식을 제공한다는 의미."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "keAPXgakhVRk",
    "outputId": "3a2c8ef8-dc62-4a52-f67b-83f4b3fba921"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=19, name='John'),\n",
       " Row(age=23, name='Smith'),\n",
       " Row(age=18, name='Sarah')]"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to create a DataFrame is to first define an RDD from a list of Rows \n",
    "some_rdd = sc.parallelize([Row(name=\"John\", age=19),\n",
    "                           Row(name=\"Smith\", age=23),\n",
    "                           Row(name=\"Sarah\", age=18)])\n",
    "some_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brpscenqhVRo",
    "outputId": "115cb8dd-c738-4ad9-96db-0dffc01b37b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The DataFrame is created from the RDD or Rows\n",
    "# Infer schema from the first row, create a DataFrame and print the schema\n",
    "some_df = sqlContext.createDataFrame(some_rdd)\n",
    "some_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5DTqPpUShVRr",
    "outputId": "4c436f6b-cd8c-48ce-e974-265ea9ded962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RDD(some_rdd) =  <class 'pyspark.rdd.RDD'> \n",
      " DataFrame(some_df) =  <class 'pyspark.sql.dataframe.DataFrame'> \n",
      "\n",
      "some_df = [Row(age=19, name='John'), Row(age=23, name='Smith'), Row(age=18, name='Sarah')]\n",
      "some_rdd= [Row(age=19, name='John'), Row(age=23, name='Smith'), Row(age=18, name='Sarah')]\n"
     ]
    }
   ],
   "source": [
    "# A dataframe is an RDD of rows plus information on the schema.\n",
    "# performing **collect()* on either the RDD or the DataFrame gives the same result.\n",
    "print(\" RDD(some_rdd) = \", type(some_rdd),\"\\n\",\"DataFrame(some_df) = \", type(some_df), \"\\n\")\n",
    "print('some_df =',some_df.collect())\n",
    "print('some_rdd=',some_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ifsfYhJKhVRu"
   },
   "source": [
    "### Defining the Schema explicitly\n",
    "The advantage of creating a DataFrame using a pre-defined schema allows the content of the RDD to be simple tuples, rather than rows.\n",
    "\n",
    "Spark가 ``schema``를 추론(inferred)하는 것이 아닌, 사용자가 직접 ``schema를 정의``할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oPyou4_whVRv",
    "outputId": "abfc1558-b76c-4938-d878-658a4d1f2e97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- person_name: string (nullable = false)\n",
      " |-- person_age: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this case we create the dataframe from an RDD of tuples (rather than Rows) and provide the schema explicitly\n",
    "another_rdd = sc.parallelize([(\"John\", 19), (\"Smith\", 23), (\"Sarah\", 18)])\n",
    "# Schema with two fields - person_name and person_age\n",
    "schema = StructType([StructField(\"person_name\", StringType(), False),\n",
    "                     StructField(\"person_age\", IntegerType(), False)])\n",
    "\n",
    "# Create a DataFrame by applying the schema to the RDD and print the schema\n",
    "another_df = sqlContext.createDataFrame(another_rdd, schema)\n",
    "another_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObbJkJodhVRy"
   },
   "source": [
    "## Loading DataFrames from disk\n",
    "There are many methods to load DataFrames from Disk. Here we will discuss three of these methods\n",
    "1. JSON (on your own)\n",
    "2. CSV  (on your own)\n",
    "3. **Parquet**\n",
    "\n",
    "In addition, there are API's for connecting Spark to an external database. We will not discuss this type of connection in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bNowuhSjhVRz"
   },
   "source": [
    "### Loading dataframes from JSON files\n",
    "[JSON](http://www.json.org/)은 ``속성-값 pair`` 또는 ``키-값 pair``으로 이루어진 데이터 오브젝트를 전달하기 위해 인간이 읽을 수 있는 텍스트를 사용하는 개방형 표준 포맷이다. 특히, 인터넷에서 자료를 주고 받을 때 그 자료를 표현하는 방법으로 알려져 있다. 자료의 종류에 큰 제한은 없으며, 특히 컴퓨터 프로그램의 변수값을 표현하는 데 적합하다. **JSON can also be used to store tabular data and can be easily loaded into a dataframe.**\n",
    "\n",
    "**( .json 예시 )**\n",
    "\n",
    "![json예시](https://wallees.files.wordpress.com/2018/04/593aa-screen2bshot2b2018-04-172bat2b4-56-002bpm.png?w=400&h=186)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jvk3aUoAhVR0",
    "outputId": "034dd034-3025-4c6e-d970-00cece90eea3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people is a <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "# people.json(예제파일 다운로드)\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1TZyM7Gfc6XWLot-L36TDV-JwySgHxGv4\", \"people.json\")\n",
    "data_file = \"./people.json\"\n",
    "\n",
    "# Create a DataFrame from the file(s) pointed to by path\n",
    "people = sqlContext.read.json(data_file)\n",
    "print('people is a',type(people))\n",
    "\n",
    "# The inferred schema can be visualized using the printSchema() method.\n",
    "people.show()\n",
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYyyfeCFhVR3"
   },
   "source": [
    "### Excercise 1 : Loading ``csv`` files into dataframes (30 point)\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "아래에 제시되는 두 가지 방법을 이용하여 ``csv``을 dataframe으로 변환하고(schema infered), ``schema``를 직접 설정하여 dataframe으로 변환합니다(**두 가지 방법 중 선택**)\n",
    "\n",
    "* 1. pandas의 ``read_csv``를 이용하여 ``csv``파일을 불러온 뒤, SQLContext의 ``createDataFrame``을 이용하여 dataframe으로 변환합니다.(10 point)\n",
    "\n",
    "* 2. SQLContext의 ``read.csv``를 이용하여 ``csv``를 dataframe으로 변환합니다. (10 point)\n",
    "\n",
    "* 3. ``schema``를 직접 설정하여 dataframe으로 변환(task1 또는 task2의 방법을 이용) (10 point)\n",
    "\n",
    "**위의 방법 외에도 ``csv``를 불러오는 방법은 많습니다. 하지만, Excercise 1 에서는 task 1, task 2 방법으로 한정합니다.**\n",
    "\n",
    "**!!csv to dataframe 방법 [참고 주소](https://stackoverflow.com/questions/29936156/get-csv-to-spark-dataframe)**\n",
    "\n",
    "- - -\n",
    "**출력 예시**\n",
    "\n",
    "* show()와 printSchema()를 이용하여 결과를 출력합니다(task 1, task 2 모두)\n",
    "\n",
    "* ex) \n",
    "\n",
    "```\n",
    "# task 1 == pandas -> createDataFrame\n",
    "\n",
    "task1.show(3)\n",
    "task1.printSchema()\n",
    "\n",
    "# task 2 == SQLContext -> DataFrame\n",
    "\n",
    "task2.show(3)\n",
    "task2.printSchema()\n",
    "\n",
    "# task 3\n",
    "task3.show(3)\n",
    "task3.printSchema()\n",
    "\n",
    "### task3.printSchema() 예시\n",
    "root\n",
    " |-- batter_id: integer (nullable = true)\n",
    " |-- batter_name: string (nullable = true)\n",
    " |-- date: string (nullable = true)\n",
    " |-- opposing_team: string (nullable = true)\n",
    " |-- avg: double (nullable = true)\n",
    " |-- AB: integer (nullable = true)\n",
    " |-- R: integer (nullable = true)\n",
    " |-- H: integer (nullable = true)\n",
    " |-- 2B: integer (nullable = true)\n",
    " |-- 3B: integer (nullable = true)\n",
    " |-- HR: integer (nullable = true)\n",
    " |-- RBI: integer (nullable = true)\n",
    " |-- CS: integer (nullable = true)\n",
    " |-- BB: integer (nullable = true)\n",
    " |-- HBP: integer (nullable = true)\n",
    " |-- SO: integer (nullable = true)\n",
    " |-- GDP: integer (nullable = true)\n",
    " |-- avg2: double (nullable = true)\n",
    " |-- year: string (nullable = true)\n",
    "```  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjPpCodThVR3",
    "outputId": "9713b042-f40f-4496-8e78-a1f65ae58e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data\t\t\t\t    Lab_HW2_20175130_박건우_upload.ipynb\r\n",
      " ex2_coalesce\t\t\t    Lec3_1_KeyValueRDD.html\r\n",
      " ex2_raw_partition\t\t    Lec3_2_execution_plan.html\r\n",
      " ex2_repartition\t\t   'Lec3_3_Word Count.html'\r\n",
      " HW1_20175130_박건우.ipynb\t    people.json\r\n",
      " hw1.svg\t\t\t    Regular_Season_Batter_Day_by_Day.csv\r\n",
      " HW3_upload_20175130_박건우.ipynb   Regular_Season_Batter_Day_by_Day.json\r\n",
      " HW3_upload_V1_링크수정.ipynb\t    save\r\n",
      " HW3_upload_V1.ipynb\t\t    test2.ipynb\r\n",
      " HW4_20175130_박건우_V1.ipynb\t    test3.ipynb\r\n",
      " HW5practice.ipynb\t\t    test.txt\r\n",
      " HW5_test.ipynb\t\t\t    Untitled1.ipynb\r\n",
      " HW5_upload_V2.ipynb\t\t    Untitled.ipynb\r\n",
      " HW5_upload_V4.ipynb\t\t    users.parquet\r\n",
      " kddcup.data_10_percent_corrected\r\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "# people.json(예제파일 다운로드)\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1QHSuh61Ng8JQ7JkAQfDZgLzd5auC9KAP\", \"Regular_Season_Batter_Day_by_Day.csv\")\n",
    "data_file = \"./Regular_Season_Batter_Day_by_Day.csv\"#파일을 읽어옴\n",
    "    \n",
    "!ls#내역을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tleaZbW8N-cp"
   },
   "source": [
    "**task**\n",
    "* 1. pandas의 ``read_csv``를 이용하여 ``csv``파일을 불러온 뒤, SQLContext의 ``createDataFrame``을 이용하여 dataframe으로 변환합니다.(10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDVTs9GthVR7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "|batter_id|batter_name|date|opposing_team| avg1| AB|  R|  H| 2B| 3B| HR|RBI| SB| CS| BB|HBP| SO|GDP|               avg2|year|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "|        0|   가르시아|3.24|           NC|0.333|  3|  1|  1|  0|  0|  0|  0|  0|  0|  1|  0|  1|  0|              0.333|2018|\n",
      "|        0|   가르시아|3.25|           NC|0.000|  4|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|0.14300000000000002|2018|\n",
      "|        0|   가르시아|3.27|         넥센|0.200|  5|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|0.16699999999999998|2018|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- batter_id: long (nullable = true)\n",
      " |-- batter_name: string (nullable = true)\n",
      " |-- date: double (nullable = true)\n",
      " |-- opposing_team: string (nullable = true)\n",
      " |-- avg1: string (nullable = true)\n",
      " |-- AB: long (nullable = true)\n",
      " |-- R: long (nullable = true)\n",
      " |-- H: long (nullable = true)\n",
      " |-- 2B: long (nullable = true)\n",
      " |-- 3B: long (nullable = true)\n",
      " |-- HR: long (nullable = true)\n",
      " |-- RBI: long (nullable = true)\n",
      " |-- SB: long (nullable = true)\n",
      " |-- CS: long (nullable = true)\n",
      " |-- BB: long (nullable = true)\n",
      " |-- HBP: long (nullable = true)\n",
      " |-- SO: long (nullable = true)\n",
      " |-- GDP: long (nullable = true)\n",
      " |-- avg2: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1-1 답안 작성\n",
    "import pandas as pd\n",
    "\n",
    "pandas_df =pd.read_csv('Regular_Season_Batter_Day_by_Day.csv')#csv를 읽고 pandas에 저장\n",
    "task1_DF =sqlContext.createDataFrame(pandas_df)#task1_df에 데이터프레임을 만듦\n",
    "# output\n",
    "task1_DF.show(3)#3개까지만 내용을 보여줌\n",
    "task1_DF.printSchema()#스키마를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcVK1L0eN-cs"
   },
   "source": [
    "**task**\n",
    "* 2. SQLContext의 ``read.csv``를 이용하여 ``csv``를 dataframe으로 변환합니다. (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DeMOAxJChVSA",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "|batter_id|batter_name|date|opposing_team| avg1| AB|  R|  H| 2B| 3B| HR|RBI| SB| CS| BB|HBP| SO|GDP|               avg2|year|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "|        0|   가르시아|3.24|           NC|0.333|  3|  1|  1|  0|  0|  0|  0|  0|  0|  1|  0|  1|  0|0.33299999999999996|2018|\n",
      "|        0|   가르시아|3.25|           NC|0.000|  4|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|0.14300000000000002|2018|\n",
      "|        0|   가르시아|3.27|         넥센|0.200|  5|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|0.16699999999999998|2018|\n",
      "+---------+-----------+----+-------------+-----+---+---+---+---+---+---+---+---+---+---+---+---+---+-------------------+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- batter_id: string (nullable = true)\n",
      " |-- batter_name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- opposing_team: string (nullable = true)\n",
      " |-- avg1: string (nullable = true)\n",
      " |-- AB: string (nullable = true)\n",
      " |-- R: string (nullable = true)\n",
      " |-- H: string (nullable = true)\n",
      " |-- 2B: string (nullable = true)\n",
      " |-- 3B: string (nullable = true)\n",
      " |-- HR: string (nullable = true)\n",
      " |-- RBI: string (nullable = true)\n",
      " |-- SB: string (nullable = true)\n",
      " |-- CS: string (nullable = true)\n",
      " |-- BB: string (nullable = true)\n",
      " |-- HBP: string (nullable = true)\n",
      " |-- SO: string (nullable = true)\n",
      " |-- GDP: string (nullable = true)\n",
      " |-- avg2: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task2_DF = sqlContext.read.csv('Regular_Season_Batter_Day_by_Day.csv',header=True)#task2_DF에 csv를 읽고 저장\n",
    "\n",
    "# output\n",
    "task2_DF.show(3)#3번까지만 보여줌\n",
    "task2_DF.printSchema()#스키마를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_rXKn89N-cw"
   },
   "source": [
    "**task**\n",
    "* 3. ``schema``를 직접 설정하여 dataframe으로 변환(task1 또는 task2의 방법을 이용) (10 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OBy8RvihVSE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- batter_id: string (nullable = true)\n",
      " |-- batter_name: string (nullable = true)\n",
      " |-- date:: string (nullable = true)\n",
      " |-- opposing_team: string (nullable = true)\n",
      " |-- avg1:: string (nullable = true)\n",
      " |-- AB: string (nullable = true)\n",
      " |-- R: string (nullable = true)\n",
      " |-- H: string (nullable = true)\n",
      " |-- 2B: string (nullable = true)\n",
      " |-- 3B: string (nullable = true)\n",
      " |-- HR: string (nullable = true)\n",
      " |-- RBI: string (nullable = true)\n",
      " |-- SB: string (nullable = true)\n",
      " |-- CS: string (nullable = true)\n",
      " |-- BB: string (nullable = true)\n",
      " |-- HBP: string (nullable = true)\n",
      " |-- SO: string (nullable = true)\n",
      " |-- GDP: string (nullable = true)\n",
      " |-- avg2: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row, StructField, StructType, StringType, IntegerType, DoubleType\n",
    "task3_schema = StructType([StructField(\"batter_id\", StringType(), True),#스키마를 직접 입력\n",
    "                     StructField(\"batter_name\", StringType(), True),StructField(\"date:\",StringType() ,True),\n",
    "                     StructField(\"opposing_team\", StringType(), True),StructField(\"avg1:\",StringType(), True),\n",
    "                     StructField(\"AB\", StringType(), True),StructField(\"R\", StringType(), True),\n",
    "                     StructField(\"H\", StringType(), True),StructField(\"2B\", StringType(), True),\n",
    "                     StructField(\"3B\", StringType(), True),StructField(\"HR\", StringType(), True),\n",
    "                     StructField(\"RBI\", StringType(), True),StructField(\"SB\", StringType(), True),\n",
    "                     StructField(\"CS\", StringType(), True),StructField(\"BB\", StringType(), True),\n",
    "                     StructField(\"HBP\", StringType(), True),StructField(\"SO\", StringType(), True),\n",
    "                     StructField(\"GDP\", StringType(), True),StructField(\"avg2\", StringType(), True),\n",
    "                     StructField(\"year\", StringType(), True)])\n",
    "\n",
    "# output\n",
    "\n",
    "\n",
    "task3_DF=sqlContext.read.csv(data_file,task3_schema)#데이터파일과 스키마를 읽어와서 task3_DF에 저장\n",
    "task3_DF.printSchema()#스키마를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nh-gfNqNhVSH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CTk93_hhVSN"
   },
   "source": [
    "### Loading dataframes from Parquet\n",
    "[Parquet](https://en.wikipedia.org/wiki/Apache_Parquet)은 ``중첩된 데이터를 효율적으로 저장할 수 있는 컬럼 기준 저장 포맷``\n",
    "\n",
    "**자세한 설명은 아래의 주소를 참고하세요**\n",
    "\n",
    "* http://engineering.vcnc.co.kr/2018/05/parquet-and-spark/\n",
    "\n",
    "* https://medium.com/ssense-tech/csv-vs-parquet-vs-avro-choosing-the-right-tool-for-the-right-job-79c9f56914a8\n",
    "\n",
    "* http://parquet.apache.org/\n",
    "\n",
    "\n",
    "\n",
    "**( Parquet 예시 )**\n",
    "\n",
    "![json예시](http://engineering.vcnc.co.kr/images/2018/05/parquet-data-io-philadelphia-2013-8-638.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clMbY8sdhVSO",
    "outputId": "a2014e18-a3ec-45cb-9273-49cab52f64c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "f = urllib.request.urlretrieve (\"https://docs.google.com/uc?export=download&id=1FKoN6JB34LIvYF571xHrLyVFduj5n-Kj\", \"users.parquet\")\n",
    "data_file = \"./users.parquet\"\n",
    "\n",
    "df = sqlContext.read.load(data_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z5pDdyr8hVSR"
   },
   "source": [
    "## Lets have a look at a real-world dataframe\n",
    "\n",
    "This dataframe is a small part from a large dataframe (15GB) which stores meteorological data from stations around the world. We will read the dataframe from a zipped parquet file.\n",
    "\n",
    "- - -\n",
    "\n",
    "``Spark가 parquet format을 load 하는 메소드는 1줄``\n",
    "\n",
    "``parquet가 어떤 형태로 되어있는지 반드시 확인할 것!``\n",
    "\n",
    "```\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7d6Dlv0QhVSS",
    "outputId": "61f53ed8-1223-4c4c-cbf5-2c8210755e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work <class 'str'>\n",
      "/home/jovyan/work/Data\n",
      "/home/jovyan/work/Data/Weather\n"
     ]
    }
   ],
   "source": [
    "from os.path import split,join,exists\n",
    "from os import mkdir,getcwd,remove\n",
    "from glob import glob\n",
    "\n",
    "# create directory if needed\n",
    "\n",
    "notebook_dir=getcwd()\n",
    "print(notebook_dir, type(notebook_dir))\n",
    "data_dir=join(notebook_dir,'Data')\n",
    "weather_dir=join(data_dir,'Weather')\n",
    "print(data_dir)\n",
    "print(weather_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ib9aGNmhVSV",
    "outputId": "b866f245-1e1a-464d-8745-824e13002ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directory /home/jovyan/work/Data already exists\n",
      "directory /home/jovyan/work/Data/Weather already exists\n",
      "NY.\n",
      "['/home/jovyan/work/Data/Weather/NY.tgz']\n",
      "removing /home/jovyan/work/Data/Weather/NY.tgz\n"
     ]
    }
   ],
   "source": [
    "# Initializing the directory\n",
    "if exists(data_dir):\n",
    "    print('directory',data_dir,'already exists')\n",
    "else:\n",
    "    print('making',data_dir)\n",
    "    mkdir(data_dir)\n",
    "\n",
    "if exists(weather_dir):\n",
    "    print('directory',weather_dir,'already exists')\n",
    "else:\n",
    "    print('making',weather_dir)\n",
    "    mkdir(weather_dir)\n",
    "\n",
    "file_index='NY'\n",
    "zip_file='%s.tgz'%(file_index)\n",
    "print(zip_file[:-3])\n",
    "\n",
    "# For linux\n",
    "old_files='%s/%s*'%(weather_dir,zip_file[:-3])\n",
    "print(glob(old_files))\n",
    "for f in glob(old_files):\n",
    "    print('removing',f)\n",
    "#   For Linux\n",
    "    !rm -rf {f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10R1aXKyhVSZ",
    "outputId": "5dbb1000-2726-4246-cd08-5f052166461f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: googledrivedownloader in /opt/conda/lib/python3.7/site-packages (0.4)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install googledrivedownloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OYigK-ZuhVSc",
    "outputId": "83026e35-fbee-4ad7-f88a-f163a3d54f1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 1hAHV6vC6FvVgrYnoN-lR-IfH488-H121 into Data/Weather/NY.tgz... Done.\n",
      "-rwxrwxrwx 1 jovyan staff 64M Oct 28  2019 /home/jovyan/work/Data/Weather/NY.tgz\n"
     ]
    }
   ],
   "source": [
    "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
    "import tarfile\n",
    "\n",
    "gdd.download_file_from_google_drive(file_id='1hAHV6vC6FvVgrYnoN-lR-IfH488-H121',\n",
    "                                   dest_path = 'Data/Weather/NY.tgz')\n",
    "\n",
    "!ls -lh $weather_dir/$zip_file\n",
    "\n",
    "#extracting the parquet file\n",
    "#!tar zxvf {weather_dir}/{zip_file} -C {weather_dir}\n",
    "\n",
    "\n",
    "tf = tarfile.open(join(weather_dir,zip_file), mode=\"r\")\n",
    "tf.extractall(weather_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEwpbU3ChVSf",
    "outputId": "46480240-ddb6-4f42-bc98-1366f06749a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/Data/Weather/NY.parquet\n",
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather_parquet = join(weather_dir, zip_file[:-3]+'parquet')\n",
    "print(weather_parquet)\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oreWDWNwhVSi"
   },
   "source": [
    "## Writing DataFrames to CSV, JSON, PARQUET\n",
    "\n",
    "생성된 DataFrame을 ``.write.[csv/json/parquet](\"폴더이름\")`` 를 통해서 write 가능\n",
    "\n",
    "\n",
    "\n",
    "설정된 ``폴더이름``에 ``DataFrame이 파티션(분할)되어 지정한 format으로 저장됨``.\n",
    "\n",
    "```\n",
    "df.write.csv(\"폴더이름\")\n",
    "df.write.json(\"폴더이름\")\n",
    "df.write.parquet(\"폴더이름\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXVp_I6MhVSp",
    "outputId": "799a1b2c-45fd-44a9-d4c6-50e30dbb7376"
   },
   "outputs": [],
   "source": [
    "if exists(\"save\"):\n",
    "    print('save directory already exists')\n",
    "else:\n",
    "    mkdir(\"save\")\n",
    "    df.write.csv(\"save/csv_test\")\n",
    "    df.write.json(\"save/json_test\")\n",
    "    df.write.parquet(\"save/parquet_test\")\n",
    "\n",
    "!ls save # save 폴더 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zuY1RCevhVSs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnTfdvOwhVSu"
   },
   "source": [
    "### Excercise 2 : ``coalesce `` or ``repartition``!! (30 point)\n",
    "\n",
    "Spark는 기본적으로 RDD or DataFrame or 등등.. 을 생성할 때, Spark Core 개수만큼 파티션을 설정한다. 예를 들어 어떠한 RDD를 생성할 때, Spark Core가 7개라면 7개의 파티션으로 처리한다. 단, DataFrame의 size에 따라 자동변경 될 수도 있다.\n",
    "\n",
    "``coalesce``와 ``repartition``는 무엇인가....?\n",
    "\n",
    "``coalesce``와 ``repartition`` 파티션 개수를 줄이거나 늘리는 데 사용한다. \n",
    "```\n",
    "#coalesce \n",
    "coalesce (numPartitions: Int, shuffle: Boolean = false)\n",
    "```\n",
    "\n",
    "자세한 설명은 [여기1](https://thebook.io/006908/part01/ch04/02/03/02/), [여기2](https://knight76.tistory.com/entry/scala-spark%EC%97%90%EC%84%9C-partition-%EC%A4%84%EC%9D%B4%EA%B8%B0-repartition-coalesce), [여기3](https://m.blog.naver.com/PostView.nhn?blogId=8x8x8x8x8x8&logNo=220740234992&proxyReferer=https%3A%2F%2Fwww.google.com%2F)을 클릭하세요.\n",
    "\n",
    "또한, Spark의 Shuffling에 대해 이해를 해야 합니다. [여기4](https://swalloow.github.io/spark-shuffling)\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "``coalesce``와 ``repartition``을 이용하여 Exercise 1에서 생성된 DataFrame의 파티션을 수정하여 ``parquet``로 저장합니다.\n",
    "\n",
    "* 1. [Dataframe].rdd.getNumPartitions() 으로 partition 값을 확인합니다.\n",
    "\n",
    "* 2. **partition을 수정하지 않고** DataFrame을 parquet로 저장후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (5 point)\n",
    "\n",
    "* 3. ``repartition``을 이용하여 DataFrame의 **partition을 2에서 10으로 증가**시킵니다. **partition이 증가된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 point)\n",
    "\n",
    "* * 4. coalesce를 이용하여 partition의 수를 1로 감소 시킵니다. **partition이 감소된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 pt)\n",
    "\n",
    "* 5. 첨부된 링크를 이용하여 ``coalesce``와 ``repartition``의 **차이점을 2 ~ 3 문장으로 서술하세요.** (5 point)\n",
    "- - -\n",
    "**출력 예시(세부 값(주소)은 다를 수 있음)**\n",
    "```\n",
    "# task 4\n",
    "part-00000-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00001-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00002-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00003-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "part-00004-0900cf86-0c0f-47e8-b6ce-d3c6ad359e45-c000.snappy.parquet\n",
    "_SUCCESS\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oGdoOmlsN-dT"
   },
   "source": [
    "**task**\n",
    "- 1. [Dataframe].rdd.getNumPartitions() 으로 partition 값을 확인합니다.\n",
    "\n",
    "``#output``\n",
    "```\n",
    "Ex1_df partition number :  4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oGM18iLiN-dU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex1_df partition number :  4\n"
     ]
    }
   ],
   "source": [
    "# task 2 -1 답안 작성\n",
    "Ex1_df =task1_DF#위에서 했던 데이터프레임을 ex1_df에 저장\n",
    "# output\n",
    "print(\"Ex1_df partition number : \", Ex1_df.rdd.getNumPartitions())#파티션이 몇개인지 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l40GPEIeN-dX"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 2. **partition을 수정하지 않고** DataFrame을 parquet로 저장후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (5 point)\n",
    "\n",
    "``#output``\n",
    "```\n",
    "part-00000-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "part-00001-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "part-00002-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "part-00003-27992d24-df2c-4c67-849b-098a4727d5b0-c000.snappy.parquet\n",
    "_SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDF6m9i7hVSv",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/jovyan/work/ex2_raw_partition already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o62.parquet.\n: org.apache.spark.sql.AnalysisException: path file:/home/jovyan/work/ex2_raw_partition already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-724413558b21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# task 2-2 답안 작성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEx1_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ex2_raw_partition\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls ex2_raw_partition'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/jovyan/work/ex2_raw_partition already exists.;'"
     ]
    }
   ],
   "source": [
    "# task 2-2 답안 작성\n",
    "Ex1_df.write.parquet(\"ex2_raw_partition\")#\"ex2_raw_partition\"이라는 이름으로 parquet파일 저장\n",
    "\n",
    "# output\n",
    "!ls ex2_raw_partition#내용을 출력 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0GGgMJWN-da"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 3. ``repartition``을 이용하여 DataFrame의 **partition을 4에서 10으로 증가**시킵니다. **partition이 증가된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 point)\n",
    "\n",
    "``#output``\n",
    "```\n",
    "part-00000-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00001-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00002-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00003-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00004-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00005-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00006-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00007-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00008-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "part-00009-1a9b7918-f4f6-4789-ab8d-798cbb9e1feb-c000.snappy.parquet\n",
    "_SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R_iqwxehVSy",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/jovyan/work/ex2_repartition already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o99.parquet.\n: org.apache.spark.sql.AnalysisException: path file:/home/jovyan/work/ex2_repartition already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:114)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7bae9f1769cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# task 2-3 답안 작성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mEx1_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ex2_repartition\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ls ex2_repartition'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/jovyan/work/ex2_repartition already exists.;'"
     ]
    }
   ],
   "source": [
    "# task 2-3 답안 작성\n",
    "Ex1_df.repartition(10).write.parquet(\"ex2_repartition\")#파티션을 10개로 증가시키고 parquet파일로 저장\n",
    "\n",
    "# output\n",
    "!ls ex2_repartition#리파티션한 내역을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKPH87gBN-dd"
   },
   "source": [
    "**task**\n",
    "\n",
    "* 4. coalesce를 이용하여 partition의 수를 1로 감소 시킵니다. **partition이 감소된 DataFrame을 parquet로 저장** 후 ``!ls``를 이용하여 저장된 parquet 파일 목록을 출력합니다. (10 point)\n",
    "\n",
    "``#output``\n",
    "```\n",
    "part-00000-01b5e415-ddcc-4d69-9478-4e2db153f103-c000.snappy.parquet  _SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UmwwGkVohVS1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-a88a0b3d-b230-45c3-9dc5-4f0c40ecd581-c000.snappy.parquet  _SUCCESS\r\n"
     ]
    }
   ],
   "source": [
    "# task 2-4 답안 작성\n",
    "Ex1_df.coalesce(1).write.parquet(\"ex2_coalesce\")#parquet파일로 저장하되 파티션을 1로 줄임\n",
    "\n",
    "# output\n",
    "!ls ex2_coalesce#내용을 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZ53bai3hVS4"
   },
   "source": [
    "### task 2-5 답안작성 (5 point)\n",
    ": 여기에 작성하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9BTkTv6hVS5"
   },
   "outputs": [],
   "source": [
    "coalesce는 random shuffle을 하지 않고, repartition은 random shuffle을 하기 때문에 성능 상 차이점이 있다.또한,coalesce는 파티션을 감소하거나 유지할 때 사용하지만 repartition은 갯수를 늘릴 때 보통 사용된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGpRsMC8hVS7"
   },
   "source": [
    "## DataFrame Operations\n",
    "```\n",
    "df = sqlContext.read.load(weather_parquet)\n",
    "df2 = sqlContext\\\n",
    ".read.csv(\"Regular_Season_Batter_Day_by_Day.csv\", header=True)\n",
    "\n",
    "```\n",
    "\n",
    "위에서 사용했던 DataFrame을 다시 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7Eriko4hVS8",
    "outputId": "9048b632-ef33-451a-a3c2-ad87bec0353c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.load(weather_parquet)\n",
    "df2 = sqlContext\\\n",
    ".read.csv(\"Regular_Season_Batter_Day_by_Day.csv\", header=True)\n",
    "\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0qsZ0fkFhVS-"
   },
   "source": [
    "### 1. columns & dtypes\n",
    "\n",
    "``columns`` : DataFrame의 column명 반환\n",
    "\n",
    "``dtypes``  : DataFrame의 column명 및 데이터 유형 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YXNdW799hVS_",
    "outputId": "cd1fe476-63cb-4e45-ac23-edf756b9f716",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Station', 'Measurement', 'Year', 'Values', 'dist_coast', 'latitude', 'longitude', 'elevation', 'state', 'name'] \n",
      "\n",
      "[('Station', 'string'), ('Measurement', 'string'), ('Year', 'bigint'), ('Values', 'binary'), ('dist_coast', 'double'), ('latitude', 'double'), ('longitude', 'double'), ('elevation', 'double'), ('state', 'string'), ('name', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print(df.columns, \"\\n\")   # columns를 이용한 DataFrame의 column 명 추출\n",
    "print(df.dtypes)          # dtypes를 이용한 column 명 및 type 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NM8xq_7hVTC"
   },
   "source": [
    "### 2. show, select, drop, filter\n",
    "\n",
    "``show(n = 20, truncate = True or False)`` : 처음 n행 출력. truncate는 정렬 여부\n",
    "\n",
    "``select(col)``  : DataFrame의 지정된 column 반환\n",
    "\n",
    "``drop(col)``    : DataFrame의 지정된 column 삭제\n",
    "\n",
    "``filter(condition)`` : True or False로 평가되는 condition 인수에 의한 DataFrame 반환\n",
    "\n",
    "``distinct()`` : 모든 column의 모든 값이 동일한 row 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2IQOUayhVTD",
    "outputId": "225e0466-e91b-4f95-863c-9ceb0cfee9d8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|state|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1950|[6E 4B 93 4B BB 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1951|[27 4A 32 4A 28 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1952|[54 4B 60 4B 6A 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1953|[48 4A 37 4A 28 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|2000|[DE 4A D4 4A CA 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|   NY|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show\n",
    "df.show(10) # n = 10, truncate=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hqoa8x-mhVTF",
    "outputId": "7f46bb35-17b4-429a-eb04-274133a3a516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----+\n",
      "|    Station|Measurement|Year|              Values|state|\n",
      "+-----------+-----------+----+--------------------+-----+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|   NY|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|   NY|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|   NY|\n",
      "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|   NY|\n",
      "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|   NY|\n",
      "+-----------+-----------+----+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select(col)\n",
    "df.select(\"Station\", \"Measurement\", \"Year\", \"Values\", \"state\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xRLfa4pBhVTH",
    "outputId": "3da287d8-f28b-4e4f-e8e0-84e1ceb1302e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----------------+\n",
      "|    Station|Measurement|Year|              Values|       dist_coast|      latitude|         longitude|        elevation|             name|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----------------+\n",
      "|USW00094704|   PRCP_s20|1945|[00 00 00 00 00 0...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1946|[99 46 52 46 0B 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1947|[79 4C 75 4C 8F 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1948|[72 48 7A 48 85 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "|USW00094704|   PRCP_s20|1949|[BB 49 BC 49 BD 4...|361.8320007324219|42.57080078125|-77.71330261230469|208.8000030517578|DANSVILLE MUNI AP|\n",
      "+-----------+-----------+----+--------------------+-----------------+--------------+------------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop(col)\n",
    "df.drop(\"state\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiCtPGHthVTX"
   },
   "source": [
    "### 4. Aggregations\n",
    "\n",
    "* **Aggregation** can be used, in combination with built-in sparkSQL functions \n",
    "to compute statistics of a dataframe.\n",
    "\n",
    "* computation will be fast thanks to combined optimzations with database operations.\n",
    "\n",
    "* A partial list : `count(), approx_count_distinct(), avg(), max(), min()`\n",
    "\n",
    "* Of these, the interesting one is `approx_count_distinct()` which uses sampling to get an approximate count fast. (`approximation`(근사치)를 이용해 빠른 결과값 도출 가능)\n",
    "\n",
    "* pyspark에서 이용 가능한 모든 Aggregations function은 [여기서 확인 하시오!!!](http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.functions)\n",
    "\n",
    "```\n",
    "from pyspark.sql.functions import [사용하고자 하는 function]\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "아래의 예제부터는 DataFrame에서 특정 column을 선택할 때, col() 사용하여 진행하겠습니다.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iheHpmEoN-d0",
    "outputId": "5be82d2e-298b-42ce-9171-8abe39d55f78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Station: string (nullable = true)\n",
      " |-- Measurement: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- Values: binary (nullable = true)\n",
      " |-- dist_coast: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- elevation: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.load(weather_parquet)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJuFBIwfN-d3"
   },
   "source": [
    "#### (1) Count, CountDistinct, approx_count_distinct\n",
    "\n",
    "**전체 row의 수 count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DG32qSUcN-d4",
    "outputId": "0ebf30ff-a5d7-49af-bc71-620a383689cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|count(Station)|\n",
      "+--------------+\n",
      "|        168398|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, countDistinct, approx_count_distinct\n",
    "\n",
    "df.select(count(col(\"Station\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aqe4LzTIN-eA"
   },
   "source": [
    "``countDistinct``는 고유한(중복없이) row의 수를 반환합니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OKzZtWrN-eB",
    "outputId": "b352a75e-c139-4ff2-cd47-01d429ac0d63",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|count(DISTINCT Station)|\n",
      "+-----------------------+\n",
      "|                    343|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(countDistinct(col(\"Station\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpe6nZIpN-eD"
   },
   "source": [
    "approx_count_distinct는 row의 근사치를 반환합니다.\n",
    "\n",
    "* approx_count_distinct는 최대 추정 오류율(maximum estimation error)의 값을 인자로 받는데, 자신이 오류를 어디까지 허용하는지에 따라 값을 주면됩니다.\n",
    "\n",
    "* count_distinct를 통해 얻는 실제값과의 차이는 최대 추정 오류의 수치에 따라 상이합니다.\n",
    "\n",
    "* **하지만, 연산은 count_distinct보다 더 빠르게 결과를 반환합니다. 즉, 대규모 데이터셋을 파악할 때 사용될 수 있습니다.!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EUsL2QqfN-eE",
    "outputId": "a76dc001-ff9f-4000-fce0-a06c02211606",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|approx_count_distinct(Station)|\n",
      "+------------------------------+\n",
      "|                           330|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(approx_count_distinct(col(\"Station\"), 0.2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xAoYP1LNN-eH"
   },
   "source": [
    "#### (2) first, last\n",
    "첫 번째 row와 마지막 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i4GupSuuN-eI",
    "outputId": "15a5832f-4110-4b2e-af07-f46db9478be0",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+\n",
      "|first(Station, false)|last(Station, false)|\n",
      "+---------------------+--------------------+\n",
      "|          USW00094704|         USC00307664|\n",
      "+---------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(col(\"Station\")), last(col(\"Station\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQv0JsH3N-eM"
   },
   "source": [
    "#### (3) min, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XNKLHJkN-eN",
    "outputId": "67f04bb6-450d-460c-c582-488b9b6ccba6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|min(year)|max(year)|\n",
      "+---------+---------+\n",
      "|     1871|     2013|\n",
      "+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df.select(min(col(\"year\")), max(col(\"year\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4CgeJLMkN-eT"
   },
   "source": [
    "#### (4) sum, sumDistinct\n",
    "\n",
    "``sumDistinct``는 고유값(중복 없는)의 sum!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcWXVxyrN-eU",
    "outputId": "137a96f7-4ee8-43bb-9de5-1fcb20a456e3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|    sum(dist_coast)|\n",
      "+-------------------+\n",
      "|4.138962684120101E7|\n",
      "+-------------------+\n",
      "\n",
      "+------------------------+\n",
      "|sum(DISTINCT dist_coast)|\n",
      "+------------------------+\n",
      "|       76862.93162703142|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, sumDistinct\n",
    "\n",
    "df.select(sum(col(\"dist_coast\"))).show()\n",
    "\n",
    "df.select(sumDistinct(col(\"dist_coast\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2K8EVfEN-eW"
   },
   "source": [
    "#### (5) avg, alias\n",
    "\n",
    "``alias``는 SQL에서 ``as``와 동일한 기능을 합니다. 집계된 column을 재활용하기 위해 이름을 설정한다고 생각하시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "utwpxpKeN-eX",
    "outputId": "b18a7e10-6c7b-4b8f-f3f3-62c901dc7ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+\n",
      "|         avg(year)|   avg(latitude)|\n",
      "+------------------+----------------+\n",
      "|1963.4289124573927|42.6842968505041|\n",
      "+------------------+----------------+\n",
      "\n",
      "+------------------+----------------+\n",
      "|             year_|      latitude_a|\n",
      "+------------------+----------------+\n",
      "|1963.4289124573927|42.6842968505041|\n",
      "+------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "df.select(avg(col(\"year\")), avg(col(\"latitude\"))).show()\n",
    "\n",
    "# alias를 사용하게 되면...\n",
    "df.select(avg(col(\"year\")).alias(\"year_\"), \n",
    "          avg(col(\"latitude\")).alias(\"latitude_a\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2qq_MA5ChVTQ"
   },
   "source": [
    "#### (6) agg and groupby\n",
    "The method `.agg(spec)` computes a summary for each group as specified in `spec`\n",
    "The method `.groupby(col)` groups rows according the value of the column `col`.  \n",
    "\n",
    "``groupy``로 원하는 **column의 데이터를 그룹화**하고, ``agg``를 통해 여러 aggregation을 동시에 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xEaVEz7cN-ea"
   },
   "source": [
    "``agg의 사용 에시``\n",
    "```\n",
    "[DataFrame].agg({'colum1':'[aggregation]', 'coulum2':'[aggregation]', ...})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADMkGuRFN-eb",
    "outputId": "b2c23700-d65f-4db9-c3f9-d3de3117eecd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+-----------------+\n",
      "|    min(dist_coast)|         avg(year)|    max(latitude)|\n",
      "+-------------------+------------------+-----------------+\n",
      "|0.04799420014023781|1963.4289124573927|44.93579864501953|\n",
      "+-------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'year':'avg', 'latitude':'max', 'dist_coast':'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_c6uxM7ThVTR",
    "outputId": "d38766b5-01f2-4781-85ca-f4ece25dbd7f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------+\n",
      "|Measurement|min(year)|count(state)|\n",
      "+-----------+---------+------------+\n",
      "|   TMIN_s20|     1873|       13442|\n",
      "|       TMIN|     1873|       13442|\n",
      "|   SNOW_s20|     1884|       15629|\n",
      "|       TOBS|     1876|       10956|\n",
      "|   SNWD_s20|     1888|       14617|\n",
      "|   PRCP_s20|     1871|       16118|\n",
      "|   TOBS_s20|     1876|       10956|\n",
      "|       TMAX|     1873|       13437|\n",
      "|       SNOW|     1884|       15629|\n",
      "|   TMAX_s20|     1873|       13437|\n",
      "|       SNWD|     1888|       14617|\n",
      "|       PRCP|     1871|       16118|\n",
      "+-----------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(col('Measurement')).agg({'year': 'min', 'state':'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQpm6H0yhVTS",
    "outputId": "9d17c325-46bc-4e03-e84c-1ce82ed953ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|    Station|min(year)|\n",
      "+-----------+---------+\n",
      "|USC00303955|     1992|\n",
      "|USW00093732|     1958|\n",
      "|USW00014786|     1945|\n",
      "|USC00300621|     1950|\n",
      "|USC00301387|     1926|\n",
      "|USC00305426|     1896|\n",
      "|USC00306659|     1898|\n",
      "|USC00303124|     1971|\n",
      "|USC00303983|     1950|\n",
      "|USC00300343|     1895|\n",
      "|USC00305441|     1973|\n",
      "|USC00303050|     1948|\n",
      "|USC00300360|     1907|\n",
      "|USW00004742|     1956|\n",
      "|USC00301401|     1902|\n",
      "|USC00306817|     1893|\n",
      "|USC00308104|     1901|\n",
      "|USC00305769|     1985|\n",
      "|USC00303889|     1926|\n",
      "|USC00306019|     1942|\n",
      "+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby(col('Station')).agg({'year': 'min'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YeGMzwjfhVTV",
    "outputId": "d8346c81-5de6-4db6-b125-9fb92130ec50",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+-----------------+\n",
      "|state|   avg(latitude)|   avg(longitude)|\n",
      "+-----+----------------+-----------------+\n",
      "|   NY|42.6842968505041|-75.4551864389521|\n",
      "+-----+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('state').agg({'latitude':'mean', 'longitude':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l0w1mntDhVTJ"
   },
   "source": [
    "#### (7) describe()\n",
    "\n",
    "The method `df.describe()` computes five statistics for each column of the dataframe `df`.\n",
    "\n",
    "    The statistics are: **count, mean, std, min,max**\n",
    "\n",
    "R의 summary와 같은 기능. 각 column의 통계치를 간단하게 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fI6mRDH6hVTJ",
    "outputId": "0e1da310-c3bd-4f9d-89c4-b03879828331",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+------------------+-------------------+------------------+------------------+------------------+------+---------------+\n",
      "|summary|    Station|Measurement|              Year|         dist_coast|          latitude|         longitude|         elevation| state|           name|\n",
      "+-------+-----------+-----------+------------------+-------------------+------------------+------------------+------------------+------+---------------+\n",
      "|  count|     168398|     168398|            168398|             168398|            168398|            168398|            168398|168398|         168398|\n",
      "|   mean|       null|       null|1963.4289124573927| 245.78455113006692|  42.6842968505041| -75.4551864389521| 245.2899639266881|  null|           null|\n",
      "| stddev|       null|       null| 30.58676603214533|  129.9711278397269|1.0492530244970495|1.7907915903419556|189.69342701097085|  null|           null|\n",
      "|    min|USC00300015|       PRCP|              1871|0.04799420014023781| 39.79999923706055|-79.58560180664062|-999.9000244140625|    NY|      ADAMS CTR|\n",
      "|    max|USW00094794|   TOBS_s20|              2013| 476.80999755859375| 44.93579864501953|-71.94999694824219| 838.2000122070312|    NY|YOUNGSTOWN 2 NE|\n",
      "+-------+-----------+-----------+------------------+-------------------+------------------+------------------+------------------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()    # 전체 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyok4C8bhVTO",
    "outputId": "08d1b696-e1cf-4283-ac7f-82107c78b339",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+\n",
      "|summary|    station|              year|\n",
      "+-------+-----------+------------------+\n",
      "|  count|     168398|            168398|\n",
      "|   mean|       null|1963.4289124573927|\n",
      "| stddev|       null| 30.58676603214533|\n",
      "|    min|USC00300015|              1871|\n",
      "|    max|USW00094794|              2013|\n",
      "+-------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().select(col('summary'),col('station'),\n",
    "                    col('year')).show() # select를 이용한 year column 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5E2Qjx6ShVTZ"
   },
   "source": [
    "### 5. Join\n",
    "\n",
    "**왼쪽과 오른쪽**의 데이터셋에 있는 ``하나 이상의 key(키)값``을 비교하고 왼쪽과 오른쪽 데이터셋의 결합 여부를 결정하는 ``조인 표현식(join expression)``의 평가 결과에 따라 두 개의 데이터셋를 조인합니다.\n",
    "\n",
    "- - -\n",
    "\n",
    "**조인 타입**\n",
    "\n",
    "```\n",
    "- 내부 조인(inner join) : 왼쪽과 오른쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 외부 조인(outer join) : 왼쪽이나 오른쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 왼쪽 외부 조인(left outer join) : 왼쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 오른쪽 외부 조인(right outer join) : 오른쪽 데이터셋에 키가 있는 row를 유지\n",
    "- 왼쪽 세미 조인(left semi join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 있는 경우에는 키가 일치하는 왼쪽 데이터셋만 유지\n",
    "- 왼쪽 안티 조인(left anti join) : 왼쪽 데이터셋의 키가 오른쪽 데이터셋에 없는 경우에는 키가 일치하지 않는 왼쪽 데이터셋만 유지\n",
    "- 자연 조인(natural join) : 두 데이터셋에서 동일한 이름을 가진 column을 암시적(implicit)으로 결합하는 조인\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PaKrnZThVTZ"
   },
   "outputs": [],
   "source": [
    "#### 예제 수행을 위한 DataFrame 생성.... 실제로는 DataFrame을 만드는 경우는 거의 없다...!\n",
    "\n",
    "person_RDD = sc.parallelize([Row(ID=0, NAME=\"Bill Chambers\",GRADUATE_PROGRAM=0,\n",
    "                                 spark_status = [100]),\n",
    "                             Row(ID=1, NAME=\"Matei Zaharia\",GRADUATE_PROGRAM=1,\n",
    "                                 spark_status = [500, 250, 100]),\n",
    "                             Row(ID=2, NAME=\"Michael Armbrust\",GRADUATE_PROGRAM=1,\n",
    "                                 spark_status = [250, 100])\n",
    "                            ])\n",
    "graduateProgram_RDD = sc.parallelize([Row(ID=0, DEGREE=\"Masters\",\n",
    "                                      DEPARTMENT=\"School of Information\",\n",
    "                                      SCHOLL = \"UC Berkeley\"),\n",
    "                                  Row(ID=2, DEGREE=\"Masters\",\n",
    "                                      DEPARTMENT=\"EECS\",\n",
    "                                      SCHOLL = \"UC Berkeley\"),\n",
    "                                  Row(ID=1, DEGREE=\"PH.D\",\n",
    "                                      DEPARTMENT=\"EECS\",\n",
    "                                      SCHOLL = \"UC Berkeley\"),\n",
    "                            ])\n",
    "spark_status_RDD = sc.parallelize([Row(ID=500, status=\"Vice President\"),\n",
    "                                   Row(ID=250, status=\"PMC Member\"),\n",
    "                                   Row(ID=100, status=\"Contributor\")\n",
    "                                  ])\n",
    "\n",
    "\n",
    "person_DF = sqlContext.createDataFrame(person_RDD)\n",
    "graduateProgram_DF = sqlContext.createDataFrame(graduateProgram_RDD)\n",
    "sparkStatus_DF = sqlContext.createDataFrame(spark_status_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DCSWLRLhVTc",
    "outputId": "c7a32141-cab4-4aa8-b95a-6806cbf5005a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person's schema\n",
      "root\n",
      " |-- GRADUATE_PROGRAM: long (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- spark_status: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n",
      "graduateProgram's schema\n",
      "root\n",
      " |-- DEGREE: string (nullable = true)\n",
      " |-- DEPARTMENT: string (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- SCHOLL: string (nullable = true)\n",
      "\n",
      "sparkStatus's schema\n",
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"person's schema\")\n",
    "person_DF.printSchema()\n",
    "print(\"graduateProgram's schema\")\n",
    "graduateProgram_DF.printSchema()\n",
    "print(\"sparkStatus's schema\")\n",
    "sparkStatus_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "igWmoyb8hVTe"
   },
   "source": [
    "### (1) inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5rnG2XjThVTh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# joinExpression 생성. 키값 설정!\n",
    "join_ex = person_DF[\"GRADUATE_PROGRAM\"] == graduateProgram_DF[\"ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrqTOLRJhVTj",
    "outputId": "7edcfcb0-3a21-40ce-92c2-9d90d190ce5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|GRADUATE_PROGRAM| ID|            NAME|   spark_status| DEGREE|          DEPARTMENT| ID|     SCHOLL|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|               0|  0|   Bill Chambers|          [100]|Masters|School of Informa...|  0|UC Berkeley|\n",
      "|               1|  1|   Matei Zaharia|[500, 250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "|               1|  2|Michael Armbrust|     [250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "+----------------+---+----------------+---------------+-------+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_DF.join(graduateProgram_DF,join_ex).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6pwjvgGhVTm"
   },
   "source": [
    "### (2) outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aehrDpF6hVTo",
    "outputId": "d5d0929e-2942-4de5-9bd3-a2592e91fb00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|GRADUATE_PROGRAM|  ID|            NAME|   spark_status| DEGREE|          DEPARTMENT| ID|     SCHOLL|\n",
      "+----------------+----+----------------+---------------+-------+--------------------+---+-----------+\n",
      "|               0|   0|   Bill Chambers|          [100]|Masters|School of Informa...|  0|UC Berkeley|\n",
      "|               1|   1|   Matei Zaharia|[500, 250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "|               1|   2|Michael Armbrust|     [250, 100]|   PH.D|                EECS|  1|UC Berkeley|\n",
      "|            null|null|            null|           null|Masters|                EECS|  2|UC Berkeley|\n",
      "+----------------+----+----------------+---------------+-------+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "person_DF.join(graduateProgram_DF, join_ex, \"outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_YJW3TdhVTu"
   },
   "source": [
    "### (3) left outer join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEzIdcovhVTv",
    "outputId": "ee57e603-a8b6-4f9d-c243-61f7874a3d0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-----------+----------------+----+----------------+---------------+\n",
      "| DEGREE|          DEPARTMENT| ID|     SCHOLL|GRADUATE_PROGRAM|  ID|            NAME|   spark_status|\n",
      "+-------+--------------------+---+-----------+----------------+----+----------------+---------------+\n",
      "|Masters|School of Informa...|  0|UC Berkeley|               0|   0|   Bill Chambers|          [100]|\n",
      "|   PH.D|                EECS|  1|UC Berkeley|               1|   1|   Matei Zaharia|[500, 250, 100]|\n",
      "|   PH.D|                EECS|  1|UC Berkeley|               1|   2|Michael Armbrust|     [250, 100]|\n",
      "|Masters|                EECS|  2|UC Berkeley|            null|null|            null|           null|\n",
      "+-------+--------------------+---+-----------+----------------+----+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram_DF.join(person_DF, join_ex, \"left_outer\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUpqJZaIhVTy"
   },
   "source": [
    "### (4) left semi join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKgu6FrfhVTz",
    "outputId": "6a54b8b4-031d-4142-d218-218456d8387f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+---+-----------+\n",
      "| DEGREE|          DEPARTMENT| ID|     SCHOLL|\n",
      "+-------+--------------------+---+-----------+\n",
      "|Masters|School of Informa...|  0|UC Berkeley|\n",
      "|   PH.D|                EECS|  1|UC Berkeley|\n",
      "+-------+--------------------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram_DF.join(person_DF, join_ex, \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0rAt9z6hVT3"
   },
   "source": [
    "### (5) left anti join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdtBzSR8hVT4",
    "outputId": "82f16d3b-e359-4f67-d270-471347e809a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---+-----------+\n",
      "| DEGREE|DEPARTMENT| ID|     SCHOLL|\n",
      "+-------+----------+---+-----------+\n",
      "|Masters|      EECS|  2|UC Berkeley|\n",
      "+-------+----------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graduateProgram_DF.join(person_DF, join_ex, \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kbE4zvfihVUF"
   },
   "source": [
    "## 6. ★★UDF(사용자 정의 함수)..★\n",
    "\n",
    "파이썬과 외부 라이브러리를 사용해서 사용자가 원하는 형태로 transformation 할 수 있다.\n",
    "\n",
    "**UDF는 하나 이상의 column을 입력으로 받고, 사용자 정의에 따라 return**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTjjxXgghVUG",
    "outputId": "9f15f207-8326-4e63-e7db-6e998295af1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|num1|num2|\n",
      "+----+----+\n",
      "|   0|   0|\n",
      "|   1|   1|\n",
      "|   2|   4|\n",
      "|   3|   9|\n",
      "|   4|  16|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF = sqlContext\\\n",
    ".createDataFrame(sc.parallelize([Row(num1 = x, num2 = x*x) for x in range(5)]))\n",
    "udfExamDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTuXtIOXhVUN"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "\n",
    "def power10(double_value):\n",
    "    return double_value ** 10\n",
    "\n",
    "power3udf = udf(power3)  # python 함수를 udf로 등록\n",
    "power10udf = udf(power10)  # python 함수를 udf로 등록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C780u4w3hVUP",
    "outputId": "d994061d-1547-42f2-ae4d-5403dd361465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+\n",
      "|power3(num1)|num2|\n",
      "+------------+----+\n",
      "|           0|   0|\n",
      "|           1|   1|\n",
      "|           8|   4|\n",
      "|          27|   9|\n",
      "|          64|  16|\n",
      "+------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF.select(power3udf(col('num1')), col('num2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SB7mi38yhVUR",
    "outputId": "8eeb69ad-0c78-426a-feb4-534b8077addd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|power3(num1)|power10(num2)|\n",
      "+------------+-------------+\n",
      "|           0|            0|\n",
      "|           1|            1|\n",
      "|           8|      1048576|\n",
      "|          27|   3486784401|\n",
      "|          64|1099511627776|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF.select(power3udf(col('num1')),power10udf(col('num2'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "narRT_AshVUT",
    "outputId": "d2b337e9-d216-4342-d19b-6bf53b9ca1f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------------+-------------+------------+-------------+\n",
      "|num1|num2|power3(num1)|power10(num1)|power3(num2)|power10(num2)|\n",
      "+----+----+------------+-------------+------------+-------------+\n",
      "|   0|   0|           0|            0|           0|            0|\n",
      "|   1|   1|           1|            1|           1|            1|\n",
      "|   2|   4|           8|         1024|          64|      1048576|\n",
      "|   3|   9|          27|        59049|         729|   3486784401|\n",
      "|   4|  16|          64|      1048576|        4096|1099511627776|\n",
      "+----+----+------------+-------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "udfExamDF.select('num1', 'num2',power3udf('num1'), power10udf('num1'), \n",
    "                power3udf('num2'), power10udf('num2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZVTdD76Dj0po"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D12DTy_1hVUp"
   },
   "source": [
    "## Exercise 3 -\n",
    "\n",
    "### DataFrame을 사용하여 HW4 Exercise 4 다시 풀기! (40 point)\n",
    "\n",
    "- - -\n",
    " \n",
    "다음 데이터에 대하여 다음 과제를 수행하세요.\n",
    "\n",
    "- regular.csv : KBO에서 활약한 타자들의 역대 정규시즌 성적을 포함하여 몸무게, 키 ,생년월일 등의 기본정보\n",
    "- pre.csv : KBO에서 활약한 타자들의 **역대 시범경기(정규시즌 직전에 여는 연습경기)** 성적\n",
    "\n",
    "**위의 두 데이터는 모두 `,`로 구분되어 있습니다.**\n",
    "\n",
    " - **데이터의 자세한 설명은 다음의 링크를 참조해주세요.([여기를 눌러서 12. 데이터 설명 참고](https://dacon.io/cpt6/62885))**\n",
    " - 또한 regular.csv와 pre.csv를 직접 열어서 데이터가 어떻게 저장되어 있는지 확인해주세요.\n",
    "\n",
    "★[column type 변경 참고](https://stackoverflow.com/questions/52871560/how-to-typecast-spark-dataframe-columns-using-pyspark)★\n",
    "\n",
    "★[column name 변경 참고](https://docs.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.withcolumnrenamed?view=spark-dotnet)★\n",
    "\n",
    "- - -\n",
    "**task**\n",
    "\n",
    "- 1. pandas 또는 .read.csv를 이용하여 ``regular.csv``와 ``pre.csv``를 각각의 DataFrame으로 만듭니다.\n",
    "\n",
    "\n",
    "- 2. 생성된 각각의 DataFrame에서 **타자 이름(batter_name), 타수(AB), 안타(H)을 ``.select(col)``를 이용하여 Transformation**합니다. **단, 새로운 DataFrame의 각 column type은 string, flot 또는 double로 변환합니다.** (10 point)\n",
    "\n",
    "\n",
    "- 3. task2에서 생성된 DataFrame 각각에 ``groupby``, ``agg``, ``udf`` 또는 ``임의의 함수를 자유롭게 적용``하여 **정규 시즌과 시범경기의 평균 타율을 구한 후 선수 이름(column 1), 평균 타율(column 2)로 구성된 DataFrame으로 Transformation 합니다(regular, pre 모두).** (10 point)\n",
    "\n",
    "\n",
    "- 4. task3의 각 DataFrame(regular, pre)를 **batter_name**을 기준으로 ``join``하여,**``역대 정규시즌 평균 타율이 역대 시범경기 평균 타율보다 높은``, 선수 이름(column 1)과 해당 선수의 역대 정규시즌 평균타율(column 2)로 구성된 DataFrame으로 Transformation 합니다.**(10 point)\n",
    "\n",
    "\n",
    "- 5. task4에서 생성된 DataFrame에 ``내림차순``을 적용하여 **상위 10명의 선수의 이름과 역대 정규시즌 평균타율 출력합니다. 단, column 명을 출력 예시와 같게 변경할 것.** (10 point)\n",
    "\n",
    "---\n",
    "\n",
    "**DataFrame join 할 때...★**\n",
    "\n",
    "\n",
    "    - DataFrame의 기준이 되는 column 명(키값)이 서로 같으면 오류가 발생합니다. \n",
    "\n",
    "    - 따라서 서로 다른 column 명을 사용해야 됩니다(alias 또는 withColumnRenamed를 사용하여 column 명 변경)\n",
    "\n",
    "    - JoinExpression을 잘 활용하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oD5f-vwEN-ff"
   },
   "source": [
    "**task**\n",
    "- 1. pandas 또는 .read.csv를 이용하여 ``regular.csv``와 ``pre.csv``를 각각의 DataFrame으로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpBPA9-KhVUq"
   },
   "outputs": [],
   "source": [
    "# pre.csv download\n",
    "from pyspark.sql.functions import col, desc\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "f = urllib.request\\\n",
    ".urlretrieve (\"https://docs.google.com/uc?export=download&id=1t3icaDgI5KeNEwNmaWFOYGYQtdY8NOMm\",\n",
    "              \"regular.csv\")\n",
    "f = urllib.request\\\n",
    ".urlretrieve (\"https://docs.google.com/uc?export=download&id=1g4r8tCCocVwCg6pTWaeioMdmtETYo_cf\",\n",
    "              \"pre.csv\")\n",
    "\n",
    "reg_df = sqlContext.read.csv(\"regular.csv\", header = True)\n",
    "pre_df = sqlContext.read.csv(\"pre.csv\", header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgKsHZ8CN-fg"
   },
   "source": [
    "**task**\n",
    "\n",
    "- 2. 생성된 각각의 DataFrame에서 **타자 이름(batter_name), 타수(AB), 안타(H)을 ``.select(col)``를 이용하여 Transformation**합니다. **단, 새로운 DataFrame의 각 column type은 string, float 또는 double로 변환합니다.** (10 point)\n",
    "\n",
    "``` # output```\n",
    "```\n",
    "★★regular DataFrame★★\n",
    "+-----------+-----+----+\n",
    "|batter_name|   AB|   H|\n",
    "+-----------+-----+----+\n",
    "|   가르시아|183.0|62.0|\n",
    "|     강경학|  1.0| 0.0|\n",
    "|     강경학| 86.0|19.0|\n",
    "+-----------+-----+----+\n",
    "only showing top 3 rows\n",
    "\n",
    "root\n",
    " |-- batter_name: string (nullable = true)\n",
    " |-- AB: double (nullable = true)\n",
    " |-- H: double (nullable = true)\n",
    "\n",
    "★★pre DataFrame★★\n",
    "+-----------+----+---+\n",
    "|batter_name|  AB|  H|\n",
    "+-----------+----+---+\n",
    "|   가르시아|20.0|7.0|\n",
    "|     강경학| 2.0|0.0|\n",
    "|     강경학| 0.0|0.0|\n",
    "+-----------+----+---+\n",
    "only showing top 3 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46isUREkN-fh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★regular DataFrame★★\n",
      "+-----------+-----+----+\n",
      "|batter_name|   AB|   H|\n",
      "+-----------+-----+----+\n",
      "|   가르시아|183.0|62.0|\n",
      "|     강경학|  1.0| 0.0|\n",
      "|     강경학| 86.0|19.0|\n",
      "+-----------+-----+----+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- batter_name: string (nullable = true)\n",
      " |-- AB: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      "\n",
      "★★pre DataFrame★★\n",
      "+-----------+----+---+\n",
      "|batter_name|  AB|  H|\n",
      "+-----------+----+---+\n",
      "|   가르시아|20.0|7.0|\n",
      "|     강경학| 2.0|0.0|\n",
      "|     강경학| 0.0|0.0|\n",
      "+-----------+----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5-2 답안 작성 \n",
    "reg_task2 = reg_df.select(reg_df.batter_name.cast(\"string\"),reg_df.AB.cast(\"double\"),reg_df.H.cast(\"double\"))#reg_task2에 스키마를  String double double형태로 바꿈\n",
    "pre_task2 = pre_df.select(pre_df.batter_name.cast(\"string\"),pre_df.AB.cast(\"double\"),pre_df.H.cast(\"double\")) #pre_task2에 스키마를 String,double,double형태로 바꿈\n",
    "\n",
    "# output\n",
    "print(\"★★regular DataFrame★★\")#regular DataFrame 출력문\n",
    "reg_task2.show(3)#task2의 3번쨰 내용까지만 보여줌\n",
    "reg_task2.printSchema()#reg_task2의 내용을 출력\n",
    "print(\"★★pre DataFrame★★\")#pre DataFrame출력문\n",
    "pre_task2.show(3)#pre_task2의 내용을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laJkBNm0N-fi"
   },
   "source": [
    "**task**\n",
    "- 3. task2에서 생성된 각각의 DataFrame에 ``groupby``, ``agg``, ``udf`` 또는 ``임의의 함수를 자유롭게 적용``하여 **역대 정규 시즌과 시범경기의 평균 타율을 구한 후 선수 이름(column 1), 평균 타율(column 2)로 구성된 DataFrame으로 Transformation 합니다(regular, pre 모두). 단, 각각의 DataFrame의 batter_name(선수 이름)과 avg(평균 타율) column의 이름을 regular/pre_batter, regular/pre_avg로 변경할 것(``alias`` 또는 ``withColumnRenamed`` 사용),** (10 point)\n",
    "\n",
    "``` # output ```\n",
    "```\n",
    "★★regular DataFrame★★\n",
    "+--------------+-------------------+\n",
    "|regular_batter|        regular_avg|\n",
    "+--------------+-------------------+\n",
    "|        김하성| 0.2879359095193214|\n",
    "|        도태훈|0.20454545454545456|\n",
    "|        이종범| 0.2965346534653465|\n",
    "+--------------+-------------------+\n",
    "only showing top 3 rows\n",
    "\n",
    "★★pre DataFrame★★\n",
    "+----------+-------------------+\n",
    "|pre_batter|            pre_avg|\n",
    "+----------+-------------------+\n",
    "|    김하성|0.19327731092436976|\n",
    "|    도태훈|0.22727272727272727|\n",
    "|    정상호|0.22674418604651161|\n",
    "+----------+-------------------+\n",
    "only showing top 3 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsilB-cmN-fj"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'gropuby'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-7ac66c4d341f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#reg_task3 = reg_task2.[빈칸3]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mreg_task3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_task2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgropuby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batter_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#pre_task3 = pre_task2.[빈칸4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mpre_task3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_task2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'batter_name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m             raise AttributeError(\n\u001b[0;32m-> 1301\u001b[0;31m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[1;32m   1302\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'gropuby'"
     ]
    }
   ],
   "source": [
    "# 5-3 답안 작성\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "def avgHit(AB_sum, H_sum):\n",
    "    return H_sum/AB_sum\n",
    "#avgHit_udf = [빈칸2]\n",
    "avgHit_udf = udf(avgHit)\n",
    "\n",
    "\n",
    "#reg_task3 = reg_task2.[빈칸3]\n",
    "reg_task3 = reg_task2.gropuby(col('batter_name')).show()\n",
    "#pre_task3 = pre_task2.[빈칸4]\n",
    "pre_task3 = pre_task2.groupby(col('batter_name')).show()\n",
    "\n",
    "# output\n",
    "print(\"★★regular DataFrame★★\")\n",
    "reg_task3.show(3)\n",
    "print(\"★★pre DataFrame★★\")\n",
    "pre_task3.show(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "44A79sdTN-fn"
   },
   "source": [
    "**task**\n",
    "- 4. task3의 각 DataFrame(regular, pre)을 **batter_name**을 기준으로 ``join``하여,**``역대 정규시즌 평균 타율이 역대 시범경기 평균 타율보다 높은``, 선수 이름(column 1)과 해당 선수의 역대 정규시즌 평균타율(column 2)로 구성된 DataFrame으로 Transformation 합니다.**(10 point)\n",
    "\n",
    "``` # Output ```\n",
    "```\n",
    "★★regular and pre joined★★\n",
    "+--------------+-------------------+\n",
    "|regular_batter|        regular_avg|\n",
    "+--------------+-------------------+\n",
    "|        김하성| 0.2879359095193214|\n",
    "|        정상호|0.24957264957264957|\n",
    "|      스크럭스| 0.2771855010660981|\n",
    "|        지석훈| 0.2260519247985676|\n",
    "|        박경수| 0.2566744730679157|\n",
    "|        박정음|0.27440633245382584|\n",
    "|        박병호| 0.2893900889453621|\n",
    "+--------------+-------------------+\n",
    "only showing top 7 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0NUTfL0N-fn"
   },
   "outputs": [],
   "source": [
    "# 5-4 답안 작성\n",
    "joinEx = [빈칸1] #joinexpresion\n",
    "task4 = reg_task3.[빈칸2]\n",
    "\n",
    "# output\n",
    "print(\"★★regular and pre joined★★\")\n",
    "task4.show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7c_lPNmN-fp"
   },
   "source": [
    "**task**\n",
    "- 5. task4에서 생성된 DataFrame에 ``내림차순``을 적용하여 **상위 10명의 선수의 이름과 역대 정규시즌 평균타율 출력합니다. 단, column 명을 출력 예시와 같게 변경할 것.** (10 point)\n",
    "\n",
    "``` # Output ```\n",
    "```\n",
    "+---------+-----------------------+\n",
    "|선수 이름|역대 정규시즌 평균 타율|\n",
    "+---------+-----------------------+\n",
    "|   장승현|    0.38461538461538464|\n",
    "|   전병우|    0.36363636363636365|\n",
    "|   이정후|    0.33827893175074186|\n",
    "|   박건우|    0.33410538506079906|\n",
    "|   김태진|     0.3333333333333333|\n",
    "|   구자욱|    0.33191489361702126|\n",
    "|   손아섭|    0.32515082171832743|\n",
    "|   김태균|     0.3247439180537772|\n",
    "|   박민우|    0.32383536861148804|\n",
    "|   김현수|     0.3226377517149812|\n",
    "+---------+-----------------------+\n",
    "only showing top 10 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hR8UJiDwN-fp"
   },
   "outputs": [],
   "source": [
    "# 5-5 답안 작성\n",
    "task5 = task4.[빈칸1]\n",
    "\n",
    "# output\n",
    "task5.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYFlyqkSN-fr"
   },
   "source": [
    "## *수고! ㅋ*"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6NM8xq_7hVTC",
    "2qq_MA5ChVTQ",
    "l0w1mntDhVTJ"
   ],
   "name": "HW5_upload_V3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
